
# 2 具体实现类

在第一节中，只是浅显的分析了大致的流程，也就是：先创建一个 ShuffleManager, 再基于它，创建一个 ShuffleWriter, 最后调用它的 write 方法将数据写出。

那么，接下来，就具体分析它们的实现类

## 2.1 SortShuffleManager

ShuffleManager --> SortShuffleManager

SortShuffleManager.scala

```scala
/**
 * In sort-based shuffle, incoming records are sorted according to their target partition ids, then
 * written to a single map output file. Reducers fetch contiguous regions of this file in order to
 * read their portion of the map output. In cases where the map output data is too large to fit in
 * memory, sorted subsets of the output can be spilled to disk and those on-disk files are merged
 * to produce the final output file.
 * 在基于排序的 shuffle 中，传来的记录根据它们的目标分区 ids 排序，然后，写入到一个 map 输出文件中。
 * Reducers 获取这个文件的连续区域，为读取这个 map 输出的一部分。
 * 如果 map 输出数据太大，以致于不能装入内存，这个输出的有序子集可以被溢写到磁盘，然后再合并这些磁盘上的文件，产生最终的输出文件。
 * 
 * Sort-based shuffle has two different write paths for producing its map output files:
 * 基于排序的 shuffle 有两个不同的写路径，来产生它的 map 输出文件：
 * - 序列化的排序：当下列三个条件都满足时，使用它   【见 canUseSerializedShuffle 这个方法】
 *   1.  shuffle 依赖没有指定 map 端的聚合
 *   2.  shuffle 序列化器支持序列化值的重定位（当前，这由 KryoSerializer 和 Spark SQL 的个性化序列化器支持）
 *   3.  shuffle 产生的输出分区数小于等于 16777216
 * - 反序列化的排序：处理所有其他的情况时使用  
 *   
 *  - Serialized sorting: used when all three of the following conditions hold:
 *    1. The shuffle dependency specifies no map-side combine.
 *    2. The shuffle serializer supports relocation of serialized values (this is currently
 *       supported by KryoSerializer and Spark SQL's custom serializers).
 *    3. The shuffle produces fewer than or equal to 16777216 output partitions.
 *  - Deserialized sorting: used to handle all other cases.
 *
 * -----------------------
 * Serialized sorting mode
 * -----------------------
 *
 * In the serialized sorting mode, incoming records are serialized as soon as they are passed to the
 * shuffle writer and are buffered in a serialized form during sorting. This write path implements
 * several optimizations:
 * 在序列化的排序模式下，只要传来的记录被发送到 shuffle writer, 就被序列化，并且，在排序期间，以序列化的形式被缓存。
 * 这个写路径实现了多个优化：
 *
 *  - Its sort operates on serialized binary data rather than Java objects, which reduces memory
 *    consumption and GC overheads. This optimization requires the record serializer to have certain
 *    properties to allow serialized records to be re-ordered without requiring deserialization.
 *    See SPARK-4550, where this optimization was first proposed and implemented, for more details.
 *    - 它的排序操作是在序列化后的二进制数据上，而不是 java 对象上，这会减少内存的消耗和 GC 负载。
 *      这个优化要求 记录序列化器 有特定的属性来允许再次排序序列化后的记录，而不用反序列化。
 *     
 *  - It uses a specialized cache-efficient sorter ([[ShuffleExternalSorter]]) that sorts
 *    arrays of compressed record pointers and partition ids. By using only 8 bytes of space per
 *    record in the sorting array, this fits more of the array into cache.
 *    - 它使用一个专门的高效缓存排序器(ShuffleExternalSorter), 它排序压缩的记录指针和分区id.
 *      通过在排序的数组中仅使用8个字节的空间，这可以将更多数组装入缓存。
 *      
 *  - The spill merging procedure operates on blocks of serialized records that belong to the same
 *    partition and does not need to deserialize records during the merge.
 *    - 溢写合并操作发生在属于相同分区的序列化记录的块上，不需要在合并期间反序列化记录。
 *    
 *  - When the spill compression codec supports concatenation of compressed data, the spill merge
 *    simply concatenates the serialized and compressed spill partitions to produce the final output
 *    partition.  This allows efficient data copying methods, like NIO's `transferTo`, to be used
 *    and avoids the need to allocate decompression or copying buffers during the merge.
 *    - 当溢写压缩编解码器支持压缩数据的连接时，溢写合并操作仅需要连接序列化的和压缩的溢写分区，来产生最终的输出分区。
 *      这就允许高效的数据复制方法，比如 NIO 的 transferTo 方法，避免需要在合并期间分配解压缩或复制缓存。
 *    
 * For more details on these optimizations, see SPARK-7081.
 */
private[spark] class SortShuffleManager(conf: SparkConf) extends ShuffleManager with Logging {

  /**
   * Obtains a [[ShuffleHandle]] to pass to tasks.
   * 获取一个 ShuffleHandle 来传递任务。
   * 【根据不同的条件选择三种形式shuffle中的一种: 
   *      BypassMergeSortShuffle  
   *      SerializedShuffle  
   *      BaseShuffle】
   */
  override def registerShuffle[K, V, C](
                                         shuffleId: Int,
                                         dependency: ShuffleDependency[K, V, C]): ShuffleHandle = {
    // 2.1.1节 
    // 绕过合并排序的条件是：不做 map 端的预聚合，且shuffle输出的分区数小于等于SHUFFLE_SORT_BYPASS_MERGE_THRESHOLD(默认值是200)
    // 这两个条件都是根据 ShuffleDependency 中的属性判断的
    if (SortShuffleWriter.shouldBypassMergeSort(conf, dependency)) {
      // 如果绕过合并排序 ⬇
      // If there are fewer than spark.shuffle.sort.bypassMergeThreshold partitions and we don't
      // need map-side aggregation, then write numPartitions files directly and just concatenate
      // them at the end. This avoids doing serialization and deserialization twice to merge
      // together the spilled files, which would happen with the normal code path. The downside is
      // having multiple files open at a time and thus more memory allocated to buffers.
      /*
        如果不做 map 端的预聚合，且shuffle输出的分区数小于等于SHUFFLE_SORT_BYPASS_MERGE_THRESHOLD,
        那么直接写入 numPartitions 个文件，并在最后连接它们。
        这就避免了为将溢写文件合并在一起，做两次序列化和反序列化，这将发生在正常的代码路径。
        下游会同时打开多个文件，因此需要分配更多内存。
       */
      // 2.1.2节 BypassMergeSortShuffleHandle: 用来识别什么时候选择使用 bypass merge sort shuffle
      new BypassMergeSortShuffleHandle[K, V](
        shuffleId, dependency.asInstanceOf[ShuffleDependency[K, V, V]])
    } 
    // 2.1.3节 决定了一个 shuffle 是否应该使用一个优化的序列化 shuffle 路径
    else if (SortShuffleManager.canUseSerializedShuffle(dependency)) {
      // Otherwise, try to buffer map outputs in a serialized form, since this is more efficient:
      // 如果不使用 bypass merge sort shuffle, 就尝试以序列化形式将 map 输出缓存，因为这更高效
      // 2.1.4节 SerializedShuffleHandle: 用来识别什么时候选择使用序列化的shuffle
      new SerializedShuffleHandle[K, V](
        shuffleId, dependency.asInstanceOf[ShuffleDependency[K, V, V]])
    } else {
      // Otherwise, buffer map outputs in a deserialized form:
      // 除了以上两种情况，以反序列化形式将 map 输出缓存
      // 2.1.5节 BaseShuffleHandle: 只捕获 registerShuffle 的参数
      new BaseShuffleHandle(shuffleId, dependency)
    }
  }

  /**
   * A mapping from shuffle ids to the task ids of mappers producing output for those shuffles.
   * key - shuffle ids 
   * value - mappers的任务ids的集合
   */
  private[this] val taskIdMapsForShuffle = new ConcurrentHashMap[Int, OpenHashSet[Long]]()
  
  /** 
   * Get a writer for a given partition. Called on executors by map tasks. 
   * 为一个给定的分区创建一个 ShuffleWriter, 它在 executors 上由 map tasks 调用
   * */
  override def getWriter[K, V](
                                handle: ShuffleHandle,
                                mapId: Long,  // 如果使用旧的shuffle获取协议，mapId就是分区id, 否则就是这个尝试任务的id
                                context: TaskContext,
                                metrics: ShuffleWriteMetricsReporter): ShuffleWriter[K, V] = {
    // 得到这个 shuffleId 对应的 产生输出的mappers的任务ids的集合
    val mapTaskIds = taskIdMapsForShuffle.computeIfAbsent(
      handle.shuffleId, _ => new OpenHashSet[Long](16))
    // 把一次任务尝试的id放进去
    mapTaskIds.synchronized { mapTaskIds.add(mapId) }
    // 根据在注册shuffle时创建的ShuffleHandle，创建对应的 ShuffleWriter
    val env = SparkEnv.get
    handle match {
      // 2.1.6节  SerializedShuffleHandle  -->  UnsafeShuffleWriter
      case unsafeShuffleHandle: SerializedShuffleHandle[K @unchecked, V @unchecked] =>
        new UnsafeShuffleWriter(
          env.blockManager,
          context.taskMemoryManager(),
          unsafeShuffleHandle,
          mapId,
          context,
          env.conf,
          metrics,
          shuffleExecutorComponents)
      // 2.1.7节  BypassMergeSortShuffleHandle  -->  BypassMergeSortShuffleWriter
      case bypassMergeSortHandle: BypassMergeSortShuffleHandle[K @unchecked, V @unchecked] =>
        new BypassMergeSortShuffleWriter(
          env.blockManager,
          bypassMergeSortHandle,
          mapId,
          env.conf,
          metrics,
          shuffleExecutorComponents)
      //  BaseShuffleHandle  -->  SortShuffleWriter
      case other: BaseShuffleHandle[K @unchecked, V @unchecked, _] =>
        new SortShuffleWriter(other, mapId, context, shuffleExecutorComponents)
    }
  }
}
```

### 2.1.1 SortShuffleWriter

SortShuffleWriter.scala

```scala
private[spark] class SortShuffleWriter[K, V, C](
    handle: BaseShuffleHandle[K, V, C],
    mapId: Long,
    context: TaskContext,
    shuffleExecutorComponents: ShuffleExecutorComponents)
  extends ShuffleWriter[K, V] with Logging {}

private[spark] object SortShuffleWriter {
  // bypass: 绕过
  // 绕过合并排序的条件是：不做 map 端的预聚合，且shuffle输出的分区数小于等于SHUFFLE_SORT_BYPASS_MERGE_THRESHOLD(默认值是200)
  def shouldBypassMergeSort(conf: SparkConf, dep: ShuffleDependency[_, _, _]): Boolean = {
    // We cannot bypass sorting if we need to do map-side aggregation.
    // 如果要做 map 端的预聚合，就不能绕过排序
    if (dep.mapSideCombine) {
      false
    }
    // 如果不做 map 端的预聚合，就不能绕过排序
    else {
      // 2.2.1节  默认值是200
      val bypassMergeThreshold: Int = conf.get(config.SHUFFLE_SORT_BYPASS_MERGE_THRESHOLD)
      // partitioner用来分区shuffle输出的分区器
      // shuffle输出的分区数小于等于阈值的话，就返回true, 那么就绕过排序  
      dep.partitioner.numPartitions <= bypassMergeThreshold
    }
  }
}
```

#### 2.1.1.1 config.SHUFFLE_SORT_BYPASS_MERGE_THRESHOLD

config\package.scala

```scala
package object config {
  private[spark] val SHUFFLE_SORT_BYPASS_MERGE_THRESHOLD =
    ConfigBuilder("spark.shuffle.sort.bypassMergeThreshold")
      .doc("In the sort-based shuffle manager, avoid merge-sorting data if there is no " +
        "map-side aggregation and there are at most this many reduce partitions")
      .version("1.1.1")
      .intConf
      .createWithDefault(200)
}
```

### 2.1.2 BypassMergeSortShuffleHandle

SortShuffleManager.scala

```scala
/**
 * Subclass of [[BaseShuffleHandle]], used to identify when we've chosen to use the
 * bypass merge sort shuffle path.
 * 用来识别什么时候选择使用 bypass merge sort shuffle
 */
private[spark] class BypassMergeSortShuffleHandle[K, V](
  shuffleId: Int,
  dependency: ShuffleDependency[K, V, V])
  extends BaseShuffleHandle(shuffleId, dependency) {
}
```

### 2.1.3 SortShuffleManager.canUseSerializedShuffle()

SortShuffleManager.scala

```scala
  /**
   * Helper method for determining whether a shuffle should use an optimized serialized shuffle
   * path or whether it should fall back to the original path that operates on deserialized objects.
   * 决定了一个 shuffle 是否应该使用一个优化的序列化 shuffle 路径，
   * 或者，它是否应该回滚到原始的路径，即在反序列化对象上操作
   */
  def canUseSerializedShuffle(dependency: ShuffleDependency[_, _, _]): Boolean = {
    val shufId = dependency.shuffleId
    // shuffle输出的分区数
    val numPartitions = dependency.partitioner.numPartitions
    // （1）序列化器支持其序列化对象的重定位
    // supportsRelocationOfSerializedObjects: 默认false 
    //    如果此序列化器支持其序列化对象的重定位，则返回true，否则返回false。
    if (!dependency.serializer.supportsRelocationOfSerializedObjects) {
      log.debug(s"Can't use serialized shuffle for shuffle $shufId because the serializer, " +
        s"${dependency.serializer.getClass.getName}, does not support object relocation")
      false
    }
    // （2） 不要做 map 端的预聚合
    // mapSideCombine 默认false 
    else if (dependency.mapSideCombine) {
      log.debug(s"Can't use serialized shuffle for shuffle $shufId because we need to do " +
        s"map-side aggregation")
      false
    }
    // （3） shuffle输出的分区数 要小于等于 SortShuffleManager支持的shuffle输出分区的最大数
    else if (numPartitions > MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE) {
      log.debug(s"Can't use serialized shuffle for shuffle $shufId because it has more than " +
        s"$MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE partitions")
      false
    } else {
      log.debug(s"Can use serialized shuffle for shuffle $shufId")
      true
    }
  }
```

### 2.1.4 SerializedShuffleHandle

```scala
/**
 * Subclass of [[BaseShuffleHandle]], used to identify when we've chosen to use the
 * serialized shuffle.
 * 用来识别什么时候选择使用序列化的shuffle
 */
private[spark] class SerializedShuffleHandle[K, V](
  shuffleId: Int,
  dependency: ShuffleDependency[K, V, V])
  extends BaseShuffleHandle(shuffleId, dependency) {
}
```

### 2.1.5 BaseShuffleHandle

BaseShuffleHandle.scala

```scala
/**
 * A basic ShuffleHandle implementation that just captures registerShuffle's parameters.
 * 一个基本的 ShuffleHandle 实现，它只捕获 registerShuffle 的参数
 */
private[spark] class BaseShuffleHandle[K, V, C](
    shuffleId: Int,
    val dependency: ShuffleDependency[K, V, C])
  extends ShuffleHandle(shuffleId)
```

### 2.1.6 UnsafeShuffleWriter

UnsafeShuffleWriter.java

```java
public class UnsafeShuffleWriter<K, V> extends ShuffleWriter<K, V> {
    public UnsafeShuffleWriter(
            BlockManager blockManager,
            TaskMemoryManager memoryManager,
            SerializedShuffleHandle<K, V> handle,
            long mapId,
            TaskContext taskContext,
            SparkConf sparkConf,
            ShuffleWriteMetricsReporter writeMetrics,
            ShuffleExecutorComponents shuffleExecutorComponents) throws SparkException {
        final int numPartitions = handle.dependency().partitioner().numPartitions();
        // SortShuffleManager支持的shuffle输出分区的最大数
        if (numPartitions > SortShuffleManager.MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE()) {
            throw new IllegalArgumentException(
                    "UnsafeShuffleWriter can only be used for shuffles with at most " +
                            SortShuffleManager.MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE() +
                            " reduce partitions");
        }
        this.blockManager = blockManager;
        this.memoryManager = memoryManager;
        this.mapId = mapId;
        final ShuffleDependency<K, V, V> dep = handle.dependency();
        this.shuffleId = dep.shuffleId();
        this.serializer = dep.serializer().newInstance();
        this.partitioner = dep.partitioner();
        this.writeMetrics = writeMetrics;
        this.shuffleExecutorComponents = shuffleExecutorComponents;
        this.taskContext = taskContext;
        this.sparkConf = sparkConf;
        this.transferToEnabled = sparkConf.getBoolean("spark.file.transferTo", true);
        this.initialSortBufferSize =
                (int) (long) sparkConf.get(package$.MODULE$.SHUFFLE_SORT_INIT_BUFFER_SIZE());
        this.inputBufferSizeInBytes =
                (int) (long) sparkConf.get(package$.MODULE$.SHUFFLE_FILE_BUFFER_SIZE()) * 1024;
        open();
    }

    static final int DEFAULT_INITIAL_SER_BUFFER_SIZE = 1024 * 1024;
    
    private void open() throws SparkException {
        assert (sorter == null);
        sorter = new ShuffleExternalSorter(
                memoryManager,
                blockManager,
                taskContext,
                initialSortBufferSize,
                partitioner.numPartitions(),
                sparkConf,
                writeMetrics);
        // 2.1.5.2节 使用指定的缓存区大小，来创建一个字节数组输出流
        serBuffer = new MyByteArrayOutputStream(DEFAULT_INITIAL_SER_BUFFER_SIZE);
        serOutputStream = serializer.serializeStream(serBuffer);
    }
}
```

#### 2.1.5.1 new ShuffleExternalSorter()

```java
/**
 * An external sorter that is specialized for sort-based shuffle.
 * 专门用于基于排序的shuffle的一个外部sorter
 * <p>
 * Incoming records are appended to data pages. When all records have been inserted (or when the
 * current thread's shuffle memory limit is reached), the in-memory records are sorted according to
 * their partition ids (using a {@link ShuffleInMemorySorter}). The sorted records are then
 * written to a single output file (or multiple files, if we've spilled). The format of the output
 * files is the same as the format of the final output file written by
 * {@link org.apache.spark.shuffle.sort.SortShuffleWriter}: each output partition's records are
 * written as a single serialized, compressed stream that can be read with a new decompression and
 * deserialization stream.
 * 传来的记录被追加到数据页。
 * 当所有记录都被插入了（或者当达到当前线程的shuffle内存极限时），根据它们的分区id排序内存中的记录（使用ShuffleInMemorySorter）
 * 然后排序后的记录被写入到一个输出文件（如果溢写了，就写入多个文件）。
 * 输出文件的格式和最终输出文件（由SortShuffleWriter写入的）的格式相同，
 * 每个输出分区的记录作为一个序列化的、压缩的流写入，这个流可以使用一个新的解压缩和反序列化流来读取
 * <p>
 * Unlike {@link org.apache.spark.util.collection.ExternalSorter}, this sorter does not merge its
 * spill files. Instead, this merging is performed in {@link UnsafeShuffleWriter}, which uses a
 * specialized merge procedure that avoids extra serialization/deserialization.
 * 不同于 ExternalSorter, 这个 sorter 不会合并它的溢写文件。
 * 这个合并在 UnsafeShuffleWriter 中执行，它使用一个专门的合并过程，能避免额外的序列化和反序列化。
 */
final class ShuffleExternalSorter extends MemoryConsumer implements ShuffleChecksumSupport {}
```

#### 2.1.5.2 new MyByteArrayOutputStream()

UnsafeShuffleWriter.java

```java
/** 
 * Subclass of ByteArrayOutputStream that exposes `buf` directly. 
 * 可以自己设置缓存区的大小
 * 
 * ByteArrayOutputStream: 这个输出流中的数据被写入字节数组。当数据写入缓冲区时，缓冲区会自动增长
 * */
  private static final class MyByteArrayOutputStream extends ByteArrayOutputStream {
    MyByteArrayOutputStream(int size) { super(size); }
    public byte[] getBuf() { return buf; }
  }
```

### 2.1.7 BypassMergeSortShuffleWriter

BypassMergeSortShuffleWriter.java

```java
/**
 * This class implements sort-based shuffle's hash-style shuffle fallback path. This write path
 * writes incoming records to separate files, one file per reduce partition, then concatenates these
 * per-partition files to form a single output file, regions of which are served to reducers.
 * Records are not buffered in memory. It writes output in a format
 * that can be served / consumed via {@link org.apache.spark.shuffle.IndexShuffleBlockResolver}.
 * 这个类实现了，基于排序shuffle的哈希风格的shuffle回滚路径。
 * 这个写路径将传来的记录写入不同的文件，一个 reduce 分区对应一个文件，然后连接这些文件，形成一个输出文件，文件中的每个区域供 reducers 使用。
 * 记录不会被缓存在内存中。它以一种可以通过 IndexShuffleBlockResolver 被服务/消费的格式写入到输出。
 * 
 * <p>
 * This write path is inefficient for shuffles with large numbers of reduce partitions because it
 * simultaneously opens separate serializers and file streams for all partitions. As a result,
 * {@link SortShuffleManager} only selects this write path when
 * 这种写路径对具有很大数量 reduce 分区数的 shuffle 来说是不高效的，因为它为所有分区同时打开了单独的序列化器和文件流。
 * 所以，SortShuffleManager 仅在以下情况时才选择这个写路径：
 *   - 没有指定 map 端的预聚合
 *   - 分区数量小于等于 spark.shuffle.sort.bypassMergeThreshold 参数
 * 
 * <ul>
 *    <li>no map-side combine is specified, and</li>
 *    <li>the number of partitions is less than or equal to
 *      <code>spark.shuffle.sort.bypassMergeThreshold</code>.</li>
 * </ul>
 *
 */
final class BypassMergeSortShuffleWriter<K, V>
  extends ShuffleWriter<K, V>
  implements ShuffleChecksumSupport {}
```
