
在 `05_任务的调度.md` 中, driver 给 executor 发消息，启动任务, 那么 executor 需要接收

CoarseGrainedExecutorBackend.scala

```scala
private[spark] class CoarseGrainedExecutorBackend(
       //.... 
       resourceProfile: ResourceProfile)
   extends IsolatedRpcEndpoint with ExecutorBackend with Logging {

  var executor: Executor = null
  
  /**
   * Map each taskId to the information about the resource allocated to it, Please refer to
   * [[ResourceInformation]] for specifics.
   * Exposed for testing only.
   * 将每个 taskId 映射到 分配给它的资源的新上
   */
  private[executor] val taskResources = new mutable.HashMap[Long, Map[String, ResourceInformation]]
  
  override def receive: PartialFunction[Any, Unit] = {
    case LaunchTask(data) =>
      if (executor == null) {
        exitExecutor(1, "Received LaunchTask command but executor was null")
      } else {
        // 前面对任务描述进行了序列化，所以，这里要反序列化
        val taskDesc = TaskDescription.decode(data.value)
        logInfo("Got assigned task " + taskDesc.taskId)
        // 取到了和这个任务相关的资源
        taskResources(taskDesc.taskId) = taskDesc.resources
        executor.launchTask(this, taskDesc)
      }
  }
}
```
Executor.scala

```scala
/**
 * Spark executor, backed by a threadpool to run tasks.
 * Spark executor 由线程池支持运行任务。
 * 
 * This can be used with Mesos, YARN, kubernetes and the standalone scheduler.
 * An internal RPC interface is used for communication with the driver,
 * except in the case of Mesos fine-grained mode.
 */
private[spark] class Executor(
     executorId: String, executorHostname: String,
     env: SparkEnv,userClassPath: Seq[URL] = Nil,
     isLocal: Boolean = false,uncaughtExceptionHandler: UncaughtExceptionHandler = new SparkUncaughtExceptionHandler,
     resources: immutable.Map[String, ResourceInformation])
  extends Logging {
  
  private[executor] def createTaskRunner(context: ExecutorBackend,
                                         taskDescription: TaskDescription) = new TaskRunner(context, taskDescription, plugins)

  // Maintains the list of running tasks.
  // key是任务id  value是运行这个任务的 Runnable 对象
  private[executor] val runningTasks = new ConcurrentHashMap[Long, TaskRunner]

  // Start worker thread pool
  // Use UninterruptibleThread to run tasks so that we can allow running codes without being
  // interrupted by `Thread.interrupt()`. Some issues, such as KAFKA-1894, HADOOP-10622,
  // will hang forever if some methods are interrupted.
  private[executor] val threadPool = {
    val threadFactory = new ThreadFactoryBuilder()
      .setDaemon(true)
      .setNameFormat("Executor task launch worker-%d")
      .setThreadFactory((r: Runnable) => new UninterruptibleThread(r, "unused"))
      .build()
    // newCachedThreadPool: 创建一个线程池，根据需要创建新线程，但在线程可用时重用之前构造的线程
    Executors.newCachedThreadPool(threadFactory).asInstanceOf[ThreadPoolExecutor]
  }
  
  def launchTask(context: ExecutorBackend, taskDescription: TaskDescription): Unit = {
    // 取到这个任务id
    val taskId = taskDescription.taskId
    // 创建一个 Runnable 对象，运行任务
    // The Runnable interface should be implemented by any class whose instances are intended to be executed by a thread.
    val tr = createTaskRunner(context, taskDescription)
    // runningTasks是一个运行中的任务的列表
    runningTasks.put(taskId, tr)
    val killMark = killMarks.get(taskId)
    if (killMark != null) {
      tr.kill(killMark._1, killMark._2)
      killMarks.remove(taskId)
    }
    // 执行任务 
    // 在将来的某个时候执行给定的任务。任务可以在新线程中执行，也可以在现有的池线程中执行。
    threadPool.execute(tr)
    if (decommissioned) {
      log.error(s"Launching a task while in decommissioned state.")
    }
  }
}
```

Runnable 对象调用 run 方法执行线程，也就是执行任务

Executor.scala

```scala
private[spark] class Executor(
      executorId: String, executorHostname: String,
      env: SparkEnv,userClassPath: Seq[URL] = Nil,
      isLocal: Boolean = false,uncaughtExceptionHandler: UncaughtExceptionHandler = new SparkUncaughtExceptionHandler,
      resources: immutable.Map[String, ResourceInformation])
  extends Logging {

  /**
   * The task to run. This will be set in run() by deserializing the task binary coming
   * from the driver. Once it is set, it will never be changed.
   * 要运行的任务。
   * 这将在 run 方法中设置，通过反序列化来自 driver 的 task 二进制文件来设置。
   * 一旦设定，就永远不会改变。
   */
    @volatile var task: Task[Any] = _
  
    override def run(): Unit = {
       //...
        val value = Utils.tryWithSafeFinally {
          // 此时，这是在计算角色 Executor 上，而不是通信角色 CoarseGrainedExecutorBackend
          val res = task.run(
            taskAttemptId = taskId,
            attemptNumber = taskDescription.attemptNumber,
            metricsSystem = env.metricsSystem,
            cpus = taskDescription.cpus,
            resources = taskDescription.resources,
            plugins = plugins)
          threwException = false
          res
        } //....
      }
    }
```

Task.scala

```scala
private[spark] abstract class Task[T](){
  /**
   * Called by [[org.apache.spark.executor.Executor]] to run this task.
   * 由 Executor 调用来执行这个任务
   * 
   * @param taskAttemptId an identifier for this task attempt that is unique within a SparkContext.
   *                      这个任务尝试的标识，它在一个 SparkContext 中是唯一的
   * @param attemptNumber how many times this task has been attempted (0 for the first attempt)
   *                      这个任务已经尝试了多少次（0标识第一次）
   * @param resources other host resources (like gpus) that this task attempt can access
   *                  这个任务尝试访问的其他主机的资源（比如 gpus）
   * @return the result of the task along with updates of Accumulators.
   */
  final def run(
      taskAttemptId: Long,
      attemptNumber: Int,
      metricsSystem: MetricsSystem,
      cpus: Int,
      resources: Map[String, ResourceInformation],
      plugins: Option[PluginContainer]): T = {

    require(cpus > 0, "CPUs per task should be > 0")
    
    // 在任务开始时调用，向BlockManager注册该任务
    SparkEnv.get.blockManager.registerTask(taskAttemptId)
    //....
    try {
      runTask(context)  // TaskContext对象
    } catch {
      //...
    } finally {
      try {
        // Call the task completion callbacks. If "markTaskCompleted" is called twice, the second
        // one is no-op.
        context.markTaskCompleted(None)
      } finally {
        //.....
        }
      }
    }
  
  // 抽象方法。  所以每个实现类都得重写自己的运行逻辑  ShuffleMapTask  ResultTask
  // 任务在计算节点 Executor 上，但具体的运行逻辑是在 Task 的各自实现类中
  def runTask(context: TaskContext): T
}
```


