
在 Spark shuffle 中，一端的任务将数据到文件中，另一端的任务从文件读取数据。

这里，先查看写数据的任务。

在分析应用程序执行的时候, 有两种类型的任务，一个是 ShuffleMapTask, 一个是 ResultTask, 也就是在 shuffle 的两端的任务。
所以写数据的任务就是 ShuffleMapTask.

所以，这里先查看 ShuffleMapTask 中的 runTask 方法

```scala
private[spark] class ShuffleMapTask(
    stageId: Int,
    stageAttemptId: Int,
    taskBinary: Broadcast[Array[Byte]],
    partition: Partition,
    @transient private var locs: Seq[TaskLocation],
    localProperties: Properties,
    serializedTaskMetrics: Array[Byte],
    jobId: Option[Int] = None,
    appId: Option[String] = None,
    appAttemptId: Option[String] = None,
    isBarrier: Boolean = false)
  extends Task[MapStatus](stageId, stageAttemptId, partition.index, localProperties,
    serializedTaskMetrics, jobId, appId, appAttemptId, isBarrier)
  with Logging {
  
      override def runTask(context: TaskContext): MapStatus = {
        // Deserialize the RDD using the broadcast variable.
        val threadMXBean = ManagementFactory.getThreadMXBean
        val deserializeStartTimeNs = System.nanoTime()
        val deserializeStartCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {
          threadMXBean.getCurrentThreadCpuTime
        } else 0L
        val ser = SparkEnv.get.closureSerializer.newInstance()
        
        /*
           taskBinary: broadcast version of the RDD and the ShuffleDependency.
           taskBinary 是在 DAGScheduler.scala 中的 submitMissingTasks 方法里，创建的并广播了
         */
        // 反序列化rdd和依赖
        val rddAndDep = ser.deserialize[(RDD[_], ShuffleDependency[_, _, _])](
          ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)
        _executorDeserializeTimeNs = System.nanoTime() - deserializeStartTimeNs
        _executorDeserializeCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {
          threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime
        } else 0L
    
        val rdd = rddAndDep._1 // 这个rdd就是ShuffleMapStage的rdd, 也就是运行map task的rdd  （调用shuffle类算子的rdd）
        val dep = rddAndDep._2  // ShuffleDependency
        // While we use the old shuffle fetch protocol, we use partitionId as mapId in the
        // ShuffleBlockId construction.
        // 如果配置了这个参数为true，mapId就是分区id, 否则就是这个尝试任务的id
        val mapId = if (SparkEnv.get.conf.get(config.SHUFFLE_USE_OLD_FETCH_PROTOCOL)) {
          partitionId
        } else context.taskAttemptId()
        // 将特定分区中的数据写到文件中
        dep.shuffleWriterProcessor.write(rdd, dep, mapId, context, partition)
      }
  
  }
```

# 1 shuffleWriterProcessor.write()

ShuffleWriteProcessor.scala

```scala
/**
 * The interface for customizing shuffle write process. The driver create a ShuffleWriteProcessor
 * and put it into [[ShuffleDependency]], and executors use it in each ShuffleMapTask.
 * 自定义shuffle写入过程的接口。
 * driver 创建一个 ShuffleWriteProcessor, 并将它放入 ShuffleDependency，并且 executors 在每个 ShuffleMapTask 中使用它。
 */
private[spark] class ShuffleWriteProcessor extends Serializable with Logging {

  /**
   * The write process for particular partition, it controls the life circle of [[ShuffleWriter]]
   * get from [[ShuffleManager]] and triggers rdd compute, finally return the [[MapStatus]] for
   * this task.
   * 特定分区的写过程，它控制着从 ShuffleManager 获取的 ShuffleWriter 的生命周期，并触发 RDD 的计算，最终返回这个任务的 MapStatus
   */
  def write(
      rdd: RDD[_], // 这个rdd就是ShuffleMapStage的rdd, 也就是运行map task的rdd  （调用shuffle类算子的rdd）
      dep: ShuffleDependency[_, _, _],
      mapId: Long,
      context: TaskContext,
      partition: Partition): MapStatus = {
    // 1.1节  ShuffleWriter: 将 map task 中获得的记录写入到 shuffle 系统
    var writer: ShuffleWriter[Any, Any] = null
    try {
      // 1.2节  driver 使用 ShuffleManager 注册shuffle, 然后 executors(或者运行在driver本地的任务) 就可以请求读取和写数据。
      val manager = SparkEnv.get.shuffleManager
      // 为一个给定的分区创建一个 ShuffleWriter, 它在 executors 上由 map tasks 调用  
      // 此时，就创建了一个ShuffleWriter，和一个ShuffleHandle
      writer = manager.getWriter[Any, Any](
        // 1.3节 使用 ShuffleManager 注册一个shuffle, 返回一个 ShuffleHandle 来传递任务。
        dep.shuffleHandle,
        mapId,
        context,
        createMetricsReporter(context))
      // 将记录的序列写入到这个任务的输出
      writer.write(
        rdd.iterator(partition, context).asInstanceOf[Iterator[_ <: Product2[Any, Any]]])
      // 关闭这个writer
      val mapStatus = writer.stop(success = true)
      if (mapStatus.isDefined) {
        // Check if sufficient shuffle mergers are available now for the ShuffleMapTask to push
        if (dep.shuffleMergeAllowed && dep.getMergerLocs.isEmpty) {
          val mapOutputTracker = SparkEnv.get.mapOutputTracker
          val mergerLocs =
            mapOutputTracker.getShufflePushMergerLocations(dep.shuffleId)
          if (mergerLocs.nonEmpty) {
            dep.setMergerLocs(mergerLocs)
          }
        }
        // Initiate shuffle push process if push based shuffle is enabled
        // The map task only takes care of converting the shuffle data file into multiple
        // block push requests. It delegates pushing the blocks to a different thread-pool -
        // ShuffleBlockPusher.BLOCK_PUSHER_POOL.
        if (!dep.shuffleMergeFinalized) {
          manager.shuffleBlockResolver match {
            case resolver: IndexShuffleBlockResolver =>
              logInfo(s"Shuffle merge enabled with ${dep.getMergerLocs.size} merger locations " +
                s" for stage ${context.stageId()} with shuffle ID ${dep.shuffleId}")
              logDebug(s"Starting pushing blocks for the task ${context.taskAttemptId()}")
              val dataFile = resolver.getDataFile(dep.shuffleId, mapId)
              new ShuffleBlockPusher(SparkEnv.get.conf)
                .initiateBlockPush(dataFile, writer.getPartitionLengths(), dep, partition.index)
            case _ =>
          }
        }
      }
      mapStatus.get
    } catch {
      //....
    }
  }
}
```

## 1.1 ShuffleWriter

```scala
/**
 * Obtained inside a map task to write out records to the shuffle system.
 * 将 map task 中获得的记录写入到 shuffle 系统
 */
private[spark] abstract class ShuffleWriter[K, V] {
  /** 
   * Write a sequence of records to this task's output 
   * 将记录的序列写入到这个任务的输出
   * */
  @throws[IOException]
  def write(records: Iterator[Product2[K, V]]): Unit

  /** 
   * Close this writer, passing along whether the map completed 
   * 关闭这个writer, 参数标识map任务是否完成
   * */
  def stop(success: Boolean): Option[MapStatus]
}
```

## 1.2 ShuffleManager

```scala
/**
 * Pluggable interface for shuffle systems. A ShuffleManager is created in SparkEnv on the driver
 * and on each executor, based on the spark.shuffle.manager setting. The driver registers shuffles
 * with it, and executors (or tasks running locally in the driver) can ask to read and write data.
 * shuffle系统的可拔插的接口。
 * 在 SparkEnv 中，在 driver 和每个 executor 上创建 ShuffleManager, 基于 `spark.shuffle.manager` 设置。
 * driver 使用 ShuffleManager 注册shuffle, 然后 executors(或者运行在driver本地的任务) 就可以请求读取和写数据。
 * 
 * NOTE:
 * 1. This will be instantiated by SparkEnv so its constructor can take a SparkConf and
 * boolean isDriver as parameters.
 * 2. This contains a method ShuffleBlockResolver which interacts with External Shuffle Service
 * when it is enabled. Need to pay attention to that, if implementing a custom ShuffleManager, to
 * make sure the custom ShuffleManager could co-exist with External Shuffle Service.
 */
private[spark] trait ShuffleManager {
  /** 
   * Get a writer for a given partition. Called on executors by map tasks. 
   * 为一个给定的分区创建一个 ShuffleWriter, 它在 executors 上由 map tasks 调用
   * */
  def getWriter[K, V](
                       handle: ShuffleHandle,
                       mapId: Long,
                       context: TaskContext,
                       metrics: ShuffleWriteMetricsReporter): ShuffleWriter[K, V]

  /**
   * Register a shuffle with the manager and obtain a handle for it to pass to tasks.
   * 使用 ShuffleManager 注册一个shuffle, 返回一个 ShuffleHandle 来传递任务。
   */
  def registerShuffle[K, V, C](
                                shuffleId: Int,
                                dependency: ShuffleDependency[K, V, C]): ShuffleHandle
}
```

## 1.3 dep.shuffleHandle

ShuffleDependency.scala

```scala
val shuffleHandle: ShuffleHandle 
  // 使用 ShuffleManager 注册一个shuffle, 返回一个 ShuffleHandle 来传递任务。
  = _rdd.context.env.shuffleManager.registerShuffle(shuffleId, this)
```

ShuffleHandle.scala

```scala
/**
 * An opaque handle to a shuffle, used by a ShuffleManager to pass information about it to tasks.
 * 由 ShuffleManager 用来传递关于它(ShuffleManager)的信息给任务
 * 
 * @param shuffleId ID of the shuffle
 */
@DeveloperApi
abstract class ShuffleHandle(val shuffleId: Int) extends Serializable {}
```