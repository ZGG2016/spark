
[TOC]

首先结合 `通信组件1.png` 理解各组件的存在，
再结合 `通信组件2.png` 理解 RpcEndpoint及其引用和收发件箱的关系，
最后结合 `通信组件3.png` 理解整个架构流程。

首先进入到 SparkContext 中

```scala
class SparkContext(config: SparkConf) extends Logging {
  // 第一
  private var _env: SparkEnv = _

  // 第三
  // This function allows components created by SparkEnv to be mocked in unit tests:
  private[spark] def createSparkEnv(
                                     conf: SparkConf,
                                     isLocal: Boolean,
                                     listenerBus: LiveListenerBus): SparkEnv = {
    // 为driver创建一个SparkEnv (保存所有的运行时环境对象，包含了serializer, RpcEnv...)
    // 最终创建了一个 NettyRpcEnv 对象，创建一个TransportServer服务端
    SparkEnv.createDriverEnv(conf, isLocal, listenerBus, SparkContext.numDriverCores(master, conf))
  }
  
  // 第二
  // Create the Spark execution environment (cache, map output tracker, etc)
  _env = createSparkEnv(_conf, isLocal, listenerBus)
}
```

SparkEnv.scala

```scala
/**
 * :: DeveloperApi ::
 * Holds all the runtime environment objects for a running Spark instance (either master or worker),
 * including the serializer, RpcEnv, block manager, map output tracker, etc. Currently
 * Spark code finds the SparkEnv through a global variable, so all the threads can access the same
 * SparkEnv. It can be accessed by SparkEnv.get (e.g. after creating a SparkContext).
 * 为一个运行中的 Spark 实例(master或worker)，保存所有的运行时环境对象，包含了serializer, RpcEnv, block manager, map output tracke...
 * 目前 Spark 代码通过一个全局变量查找 SparkEnv, 因此所有线程都可以访问同一个 SparkEnv。
 * SparkEnv 可以通过 SparkEnv.get 访问 (例如在创建 SparkContext 之后)。
 */
@DeveloperApi
class SparkEnv (){
  object SparkEnv extends Logging {
    /**
     * Create a SparkEnv for the driver.
     */
    private[spark] def createDriverEnv(
                                        conf: SparkConf,
                                        isLocal: Boolean,
                                        listenerBus: LiveListenerBus,
                                        numCores: Int,
                                        mockOutputCommitCoordinator: Option[OutputCommitCoordinator] = None): SparkEnv = {
      assert(conf.contains(DRIVER_HOST_ADDRESS),
        s"${DRIVER_HOST_ADDRESS.key} is not set on the driver!")
      assert(conf.contains(DRIVER_PORT), s"${DRIVER_PORT.key} is not set on the driver!")
      val bindAddress = conf.get(DRIVER_BIND_ADDRESS)
      val advertiseAddress = conf.get(DRIVER_HOST_ADDRESS)
      val port = conf.get(DRIVER_PORT)
      val ioEncryptionKey = if (conf.get(IO_ENCRYPTION_ENABLED)) {
        Some(CryptoStreamUtils.createKey(conf))
      } else {
        None
      }
      // 这里   最终创建了一个 NettyRpcEnv 对象，创建一个TransportServer服务端
      create(
        conf,
        SparkContext.DRIVER_IDENTIFIER,
        bindAddress,
        advertiseAddress,
        Option(port),
        isLocal,
        numCores,
        ioEncryptionKey,
        listenerBus = listenerBus,
        mockOutputCommitCoordinator = mockOutputCommitCoordinator
      )
    }

    private def create(
                      //....
                        mockOutputCommitCoordinator: Option[OutputCommitCoordinator] = None): SparkEnv = {

      val isDriver = executorId == SparkContext.DRIVER_IDENTIFIER
      // 创建一个 NettyRpcEnv 对象，创建一个TransportServer服务端
      val rpcEnv = RpcEnv.create(systemName, bindAddress, advertiseAddress, 
                                 port.getOrElse(-1), conf, securityManager, numUsableCores, !isDriver)
      //....
    }
  }
}
```

RpcEnv.scala

```scala
private[spark] object RpcEnv {
  def create(
             //....
              clientMode: Boolean): RpcEnv = {
    val config = RpcEnvConfig(conf, name, bindAddress, advertiseAddress, port, securityManager,
      numUsableCores, clientMode)
    // 创建一个NettyRpcEnv对象，创建一个TransportServer服务端
    new NettyRpcEnvFactory().create(config)
  }
}
```
NettyRpcEnv.scala

```scala
private[rpc] class NettyRpcEnvFactory extends RpcEnvFactory with Logging {

  def create(config: RpcEnvConfig): RpcEnv = {
    val sparkConf = config.conf
    // Use JavaSerializerInstance in multiple threads is safe. However, if we plan to support
    // KryoSerializer in future, we have to use ThreadLocal to store SerializerInstance
    val javaSerializerInstance = {
      new JavaSerializer(sparkConf).newInstance().asInstanceOf[JavaSerializerInstance]
      // new一个NettyRpcEnv对象
      val nettyEnv =
        new NettyRpcEnv(sparkConf, javaSerializerInstance, config.advertiseAddress,
          config.securityManager, config.numUsableCores)
      if (!config.clientMode) {
        // 重新回到这里
        val startNettyRpcEnv: Int => (NettyRpcEnv, Int) = 
        { actualPort =>
          // 1.2节  创建一个TransportServer服务端，它将尝试绑定一个特定的主机和端口
          nettyEnv.startServer(config.bindAddress, actualPort)
          (nettyEnv, nettyEnv.address.port)
        }
        try {
          // 1.1节 尝试在给定端口上启动一个服务
          // 或在多次尝试后失败。每次后续尝试使用 1 +上一次尝试使用的端口(除非端口是0)【本次是80端口，那么下次尝试就使用81端口】。
          Utils.startServiceOnPort(config.port, startNettyRpcEnv, sparkConf, config.name)._1
        } catch {
          case NonFatal(e) =>
            nettyEnv.shutdown()
            throw e
        }
      }
      nettyEnv
    }
  }
}
```
## 1.1 Utils.startServiceOnPort()

Util.scala

```scala
def startServiceOnPort[T](
      startPort: Int,
      startService: Int => (T, Int),
      conf: SparkConf,
      serviceName: String = ""): (T, Int) = {
  // 这里的 startService 是传进来的参数，所以往会看
    val (service, port) = startService(tryPort)
    logInfo(s"Successfully started service$serviceString on port $port.")
    return (service, port)
}
```

## 1.2 nettyEnv.startServer()

NettyRpcEnv.scala

```scala
private[netty] class NettyRpcEnv(val conf: SparkConf,
                                  javaSerializerInstance: JavaSerializerInstance,
                                  host: String,
                                  securityManager: SecurityManager,
                                  numUsableCores: Int) extends RpcEnv(conf) with Logging {
  // TransportContext:包含了用于创建 TransportServer、TransportClientFactory 和使用 TransportChannelHandler 设置Netty通道管道的上下文
  // NettyRpcHandler: 将传来的 RPC 分发给注册过的 endpoint
  private val transportContext = new TransportContext(transportConf,
    new NettyRpcHandler(dispatcher, this, streamManager))

  @volatile private var server: TransportServer = _

  def startServer(bindAddress: String, port: Int): Unit = {
    val bootstraps: java.util.List[TransportServerBootstrap] = {
      //....
      // 1.2.1节 创建一个TransportServer服务端，它将尝试绑定一个特定的主机和端口
      server = transportContext.createServer(bindAddress, port, bootstraps)
      // 【1.2.2节】 注册一个 RpcEndpoint，同时也会注册一个对应的引用 endpointRef，创建一个 InBox 收件箱
      dispatcher.registerRpcEndpoint(
        RpcEndpointVerifier.NAME, new RpcEndpointVerifier(this, dispatcher))
    }
  }
}
```

### 1.2.1 transportContext.createServer()

TransportContext.java

```java
/**
 * Contains the context to create a {@link TransportServer}, {@link TransportClientFactory}, and to
 * setup Netty Channel pipelines with a
 * {@link org.apache.spark.network.server.TransportChannelHandler}.
 *
 * There are two communication protocols that the TransportClient provides, control-plane RPCs and
 * data-plane "chunk fetching". The handling of the RPCs is performed outside of the scope of the
 * TransportContext (i.e., by a user-provided handler), and it is responsible for setting up streams
 * which can be streamed through the data plane in chunks using zero-copy IO.
 *
 * The TransportServer and TransportClientFactory both create a TransportChannelHandler for each
 * channel. As each TransportChannelHandler contains a TransportClient, this enables server
 * processes to send messages back to the client on an existing channel.
 */
public class TransportContext implements Closeable {
    /**
     *  Create a server which will attempt to bind to a specific host and port. 
     *  创建一个服务端，它将尝试绑定一个特定的主机和端口
     *  */
    public TransportServer createServer(
            String host, int port, List<TransportServerBootstrap> bootstraps) {
        return new TransportServer(this, host, port, rpcHandler, bootstraps);
    }
}
```
TransportServer.java

```java
public class TransportServer implements Closeable {
    /**
     * Creates a TransportServer that binds to the given host and the given port, or to any available
     * if 0. If you don't want to bind to any special host, set "hostToBind" to null.
     * */
    public TransportServer(
            TransportContext context,
            String hostToBind,
            int portToBind,
            RpcHandler appRpcHandler,
            List<TransportServerBootstrap> bootstraps) {
        this.context = context;
        this.conf = context.getConf();
        this.appRpcHandler = appRpcHandler;
        if (conf.sharedByteBufAllocators()) {
            this.pooledAllocator = NettyUtils.getSharedPooledByteBufAllocator(
                    conf.preferDirectBufsForSharedByteBufAllocators(), true /* allowCache */);
        } else {
            this.pooledAllocator = NettyUtils.createPooledByteBufAllocator(
                    conf.preferDirectBufs(), true /* allowCache */, conf.serverThreads());
        }
        this.bootstraps = Lists.newArrayList(Preconditions.checkNotNull(bootstraps));

        boolean shouldClose = true;
        try {
            // 初始化服务端
            init(hostToBind, portToBind);
            shouldClose = false;
        } finally {
            if (shouldClose) {
                JavaUtils.closeQuietly(this);
            }
        }
    }

    private void init(String hostToBind, int portToBind) {
        /*
           conf.ioMode()      
              return conf.get(SPARK_NETWORK_IO_MODE_KEY, "NIO").toUpperCase(Locale.ROOT);
              默认NIO
         */
        IOMode ioMode = IOMode.valueOf(conf.ioMode());
        EventLoopGroup bossGroup = NettyUtils.createEventLoop(ioMode, 1,
                conf.getModuleName() + "-boss");
        EventLoopGroup workerGroup = NettyUtils.createEventLoop(ioMode, conf.serverThreads(),
                conf.getModuleName() + "-server");

        bootstrap = new ServerBootstrap()
                .group(bossGroup, workerGroup)
                // 根据设置的IOMode，选择ServerSocketChannel
                .channel(NettyUtils.getServerChannelClass(ioMode))
                .option(ChannelOption.ALLOCATOR, pooledAllocator)
                .option(ChannelOption.SO_REUSEADDR, !SystemUtils.IS_OS_WINDOWS)
                .childOption(ChannelOption.ALLOCATOR, pooledAllocator);

        //...
    }
}
```

NettyUtil.java

```java
public class NettyUtils {
    // Returns the correct ServerSocketChannel class based on IOMode.
    public static Class<? extends ServerChannel> getServerChannelClass(IOMode mode) {
        switch (mode) {
            case NIO:
                return NioServerSocketChannel.class;
            case EPOLL:
                return EpollServerSocketChannel.class;
            default:
                throw new IllegalArgumentException("Unknown io mode: " + mode);
        }
    }
}
```

### 1.2.2 dispatcher.registerRpcEndpoint()

Dispatcher.scala

```scala
private[netty] class Dispatcher(nettyEnv: NettyRpcEnv, numUsableCores: Int) extends Logging {
  
  def registerRpcEndpoint(name: String, endpoint: RpcEndpoint): NettyRpcEndpointRef = {
    val addr = RpcEndpointAddress(nettyEnv.address, name)
    // 创建终端引用
    val endpointRef = new NettyRpcEndpointRef(nettyEnv.conf, addr, nettyEnv)
    synchronized {
      if (stopped) {
        throw new IllegalStateException("RpcEnv has been stopped")
      }
      if (endpoints.containsKey(name)) {
        throw new IllegalArgumentException(s"There is already an RpcEndpoint called $name")
      }

      // This must be done before assigning RpcEndpoint to MessageLoop, as MessageLoop sets Inbox be
      // active when registering, and endpointRef must be put into endpointRefs before onStart is
      // called.
      // 注册终端引用，放环境里的一个map中
      endpointRefs.put(endpoint, endpointRef)

      var messageLoop: MessageLoop = null
      try {
        messageLoop = endpoint match {
          case e: IsolatedRpcEndpoint =>
            // 创建一个InBox收件箱
            new DedicatedMessageLoop(name, e, this)
          case _ =>
            sharedLoop.register(name, endpoint)
            sharedLoop
        }
        // 注册终端
        endpoints.put(name, messageLoop)
      } catch {
        case NonFatal(e) =>
          endpointRefs.remove(endpoint)
          throw e
      }
    }
    endpointRef
  }
}
```

再查看一下 RpcEndpoint 能干什么

所以，它有一个对应的引用，也可以接收消息并回复

```scala
private[spark] trait RpcEndpoint {
  /**
   * The [[RpcEndpointRef]] of this [[RpcEndpoint]]. `self` will become valid when `onStart` is
   * called. And `self` will become `null` when `onStop` is called.
   * 它自己对应的引用
   * 
   * Note: Because before `onStart`, [[RpcEndpoint]] has not yet been registered and there is not
   * valid [[RpcEndpointRef]] for it. So don't call `self` before `onStart` is called.
   */
  final def self: RpcEndpointRef = {
    require(rpcEnv != null, "rpcEnv has not been initialized")
    rpcEnv.endpointRef(this)
  }

  /**
   * Process messages from `RpcEndpointRef.send` or `RpcCallContext.reply`. If receiving a
   * unmatched message, `SparkException` will be thrown and sent to `onError`.
   */
  def receive: PartialFunction[Any, Unit] = {
    case _ => throw new SparkException(self + " does not implement 'receive'")
  }

  /**
   * Process messages from `RpcEndpointRef.ask`. If receiving a unmatched message,
   * `SparkException` will be thrown and sent to `onError`.
   */
  def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {
    case _ => context.sendFailure(new SparkException(self + " won't reply anything"))
  }
}
```

接下来再看一下这个引用

所以，它能发送消息

```scala
/**
 * A reference for a remote [[RpcEndpoint]]. [[RpcEndpointRef]] is thread-safe.
 */
private[spark] abstract class RpcEndpointRef(conf: SparkConf)
  extends Serializable with Logging {

  /**
   * return the address for the [[RpcEndpointRef]]
   */
  def address: RpcAddress
  
  /**
   * Send a message to the corresponding [[RpcEndpoint.receiveAndReply)]] and return a [[Future]] to
   * receive the reply within the specified timeout.
   * 发送消息给对应的 RpcEndpoint 的 receiveAndReply 方法，并返回一个 Future 来接收回复
   * 
   * This method only sends the message once and never retries.
   */
  def ask[T: ClassTag](message: Any, timeout: RpcTimeout): Future[T]
  }
```

所以，RpcEndpoint 有一个收件箱接收消息，而 RpcEndpointRef 可以发送消息。
（当从一个 RpcEndpointA 向另一个 RpcEndpointB 发送消息时，一般我们需要获取到该 RpcEndpointB 的引用，然后通过该引用发送消息。）

其实也存在一个发件箱的概念

在 NettyRpcEnv.scala 中

```scala
  /**
   * A map for [[RpcAddress]] and [[Outbox]]. When we are connecting to a remote [[RpcAddress]],
   * we just put messages to its [[Outbox]] to implement a non-blocking `send` method.
   * RpcAddress(RpcEndpointRef)到Outbox的映射。
   * (RpcEndpoint)当连接到一个远程 RpcAddress(RpcEndpointRef)时，(RpcEndpointRef)就尝试将消息放到它的Outbox，以实现一个非阻塞的 "发送" 方法
   */
  private val outboxes = new ConcurrentHashMap[RpcAddress, Outbox]()
```

Outbox.scala

```scala
private[netty] class Outbox(nettyEnv: NettyRpcEnv, val address: RpcAddress) {

  // 将消息发送给服务端的 RpcHandler
  private var client: TransportClient = _
  
  /**
   * Send a message. If there is no active connection, cache it and launch a new connection. If
   * [[Outbox]] is stopped, the sender will be notified with a [[SparkException]].
   */
  def send(message: OutboxMessage): Unit = {
    val dropped = synchronized {
      if (stopped) {
        true
      } else {
        messages.add(message)
        false
      }
    }
    if (dropped) {
      message.onFailure(new SparkException("Message is dropped because Outbox is stopped"))
    } else {
      drainOutbox()
    }
  }
}
```

TransportClient.java

```java
public class TransportClient implements Closeable {
    /**
     * Sends an opaque message to the RpcHandler on the server-side. The callback will be invoked
     * with the server's response or upon any failure.
     * 将消息发送给服务端的 RpcHandler
     * 
     * @param message The message to send.
     * @param callback Callback to handle the RPC's reply.
     * @return The RPC's id.
     */
    public long sendRpc(ByteBuffer message, RpcResponseCallback callback) {
        if (logger.isTraceEnabled()) {
            logger.trace("Sending RPC to {}", getRemoteAddress(channel));
        }

        long requestId = requestId();
        // handler 是客户端处理了服务端响应的TransportResponseHandler
        handler.addRpcRequest(requestId, callback);

        RpcChannelListener listener = new RpcChannelListener(requestId, callback);
        // 通过 channel 连接到服务端
        channel.writeAndFlush(new RpcRequest(requestId, new NioManagedBuffer(message)))
                .addListener(listener);

        return requestId;
    }
}
```

以上就是 Driver 中的通信组件。而 Executor 也有相应的通信组件。

