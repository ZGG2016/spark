
在 Spark shuffle 中，一端的任务将数据到文件中，另一端的任务从文件读取数据。

前面分析了写数据，这里分析读数据

在分析应用程序执行的时候, 有两种类型的任务，一个是 ShuffleMapTask, 一个是 ResultTask, 也就是在 shuffle 的两端的任务。
所以读数据的任务就是 ResultTask.

所以，这里先查看 ResultTask 中的 runTask 方法

ResultTask.scala

```scala
  override def runTask(context: TaskContext): U = {
    // Deserialize the RDD and the func using the broadcast variables.
    val threadMXBean = ManagementFactory.getThreadMXBean
    val deserializeStartTimeNs = System.nanoTime()
    val deserializeStartCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {
      threadMXBean.getCurrentThreadCpuTime
    } else 0L
    val ser = SparkEnv.get.closureSerializer.newInstance()
    val (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) => U)](
      ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)
    _executorDeserializeTimeNs = System.nanoTime() - deserializeStartTimeNs
    _executorDeserializeCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {
      threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime
    } else 0L
    
    func(context, rdd.iterator(partition, context))
  }
```

RDD.scala

```scala
/**
   * Internal method to this RDD; will read from cache if applicable, or otherwise compute it.
   * This should ''not'' be called by users directly, but is available for implementers of custom
   * subclasses of RDD.
   */
  final def iterator(split: Partition, context: TaskContext): Iterator[T] = {
    if (storageLevel != StorageLevel.NONE) {
      // 获取或计算RDD的分区
      getOrCompute(split, context)
    } else {
      computeOrReadCheckpoint(split, context)
    }
  }
```

通过层层调用，进入到 ShuffleRDD 

ShuffleRDD.scala

```scala
override def compute(split: Partition, context: TaskContext): Iterator[(K, C)] = {
    val dep = dependencies.head.asInstanceOf[ShuffleDependency[K, V, C]]
    val metrics = context.taskMetrics().createTempShuffleReadMetrics()
   // 获取一个 reader, 来读取 shuffle 的所有 map 输出
    SparkEnv.get.shuffleManager.getReader(  // 1.1节
      dep.shuffleHandle, split.index, split.index + 1, context, metrics)
      .read() // 1.2节
      .asInstanceOf[Iterator[(K, C)]]
  }
```

## 1.1 getReader()

ShuffleManager.scala

```scala
private[spark] trait ShuffleManager {
  /**
   * Get a reader for a range of reduce partitions (startPartition to endPartition-1, inclusive) to
   * read from all map outputs of the shuffle.
   * 获取一个 reader, 来读取 shuffle 的所有 map 输出
   * 
   * Called on executors by reduce tasks.
   */
  final def getReader[K, C](
                             handle: ShuffleHandle,
                             startPartition: Int,
                             endPartition: Int,
                             context: TaskContext,
                             metrics: ShuffleReadMetricsReporter): ShuffleReader[K, C] = {
    getReader(handle, 0, Int.MaxValue, startPartition, endPartition, context, metrics)
  }

  /**
   * Get a reader for a range of reduce partitions (startPartition to endPartition-1, inclusive) to
   * read from a range of map outputs(startMapIndex to endMapIndex-1, inclusive).
   * If endMapIndex=Int.MaxValue, the actual endMapIndex will be changed to the length of total map
   * outputs of the shuffle in `getMapSizesByExecutorId`.
   *
   * Called on executors by reduce tasks.  在 executors 上，由 reduce tasks 调用
   */
  def getReader[K, C](
                       handle: ShuffleHandle,
                       startMapIndex: Int,
                       endMapIndex: Int,
                       startPartition: Int,
                       endPartition: Int,
                       context: TaskContext,
                       metrics: ShuffleReadMetricsReporter): ShuffleReader[K, C]
}
```

查看其实现类 

SortShuffleManager.scala

```scala
override def getReader[K, C](
      handle: ShuffleHandle,
      startMapIndex: Int,
      endMapIndex: Int,
      startPartition: Int,
      endPartition: Int,
      context: TaskContext,
      metrics: ShuffleReadMetricsReporter): ShuffleReader[K, C] = {
  // BaseShuffleHandle 能捕获 registerShuffle 的参数，所以它有注册shuffle时的信息
    val baseShuffleHandle = handle.asInstanceOf[BaseShuffleHandle[K, _, C]]
    //  blocksByAddress: 块的位置信息和块信息
    //  canEnableBatchFetch: 是否启用了batch fetch
    val (blocksByAddress, canEnableBatchFetch) = {
      // 判断 shuffle map 阶段是否完成了 shuffle merge  
      // 默认false
      if (baseShuffleHandle.dependency.isShuffleMergeFinalizedMarked) {
        /*
           getMapSizesByExecutorId: 启用基于推送的shuffle时，从executor调用，以获取每个shuffle块的服务器uri和输出大小，
                                     这些shuffle块需要从一定范围的 map 输出分区中读取
         */
        val res = SparkEnv.get.mapOutputTracker.getPushBasedShuffleMapSizesByExecutorId(
          handle.shuffleId, startMapIndex, endMapIndex, startPartition, endPartition)
        (res.iter, res.enableBatchFetch)  // [BlockManagerId  ,(shuffle block id, shuffle block size, map index)]
      } else {
        /*
            MapOutputTracker: 会跟踪一个阶段的 map 输出的位置
            getMapSizesByExecutorId: 未启用基于推送的shuffle时，从executor调用，以获取每个shuffle块的服务器uri和输出大小，
                                     这些shuffle块需要从一定范围的 map 输出分区中读取
         */
        val address = SparkEnv.get.mapOutputTracker.getMapSizesByExecutorId(
          handle.shuffleId, startMapIndex, endMapIndex, startPartition, endPartition)
        (address, true)
      }
    }
  // 通过向其他节点的块存储 请求从shuffle中获取和读取块
  new BlockStoreShuffleReader(
      handle.asInstanceOf[BaseShuffleHandle[K, _, C]], blocksByAddress, context, metrics,
      shouldBatchFetch =
        canEnableBatchFetch && canUseBatchFetch(startPartition, endPartition, context))
  }
```

```scala
  /**
   * Helper method for determining whether a shuffle reader should fetch the continuous blocks
   * in batch.
   */
  def canUseBatchFetch(startPartition: Int, endPartition: Int, context: TaskContext): Boolean = {
    val fetchMultiPartitions = endPartition - startPartition > 1
    fetchMultiPartitions &&
      context.getLocalProperty(FETCH_SHUFFLE_BLOCKS_IN_BATCH_ENABLED_KEY) == "true"
  }
```

## 1.2 read()

```scala
/**
 * Fetches and reads the blocks from a shuffle by requesting them from other nodes' block stores.
 */
private[spark] class BlockStoreShuffleReader[K, C](
    handle: BaseShuffleHandle[K, _, C],
    blocksByAddress: Iterator[(BlockManagerId, Seq[(BlockId, Long, Int)])],
    context: TaskContext,
    readMetrics: ShuffleReadMetricsReporter,
    serializerManager: SerializerManager = SparkEnv.get.serializerManager,
    blockManager: BlockManager = SparkEnv.get.blockManager,
    mapOutputTracker: MapOutputTracker = SparkEnv.get.mapOutputTracker,
    shouldBatchFetch: Boolean = false)
  extends ShuffleReader[K, C] with Logging {
  
      /** 
       * Read the combined key-values for this reduce task
       * 为这个 reduce task 读取聚合的键值对
       * */
      override def read(): Iterator[Product2[K, C]] = {
        // 1.2.1节 获取多个块的迭代器
        val wrappedStreams = new ShuffleBlockFetcherIterator(
          context,
          blockManager.blockStoreClient,
          blockManager,
          mapOutputTracker,
          blocksByAddress,
          serializerManager.wrapStream,
          // Note: we use getSizeAsMb when no suffix is provided for backwards compatibility

          /*
            REDUCER_MAX_SIZE_IN_FLIGHT: spark.reducer.maxSizeInFlight
             shuffle reduce task 的 buffer 缓冲区大小决定了 reduce task 每次
              能够缓冲的数据量，也就是每次能够拉取的数据量，如果内存资源较为充足，
              适当增加拉取数据缓冲区的大小，可以减少拉取数据的次数，
              也就可以减少网络传输的次数，进而提升性能。

              缓冲区的大小，默认为 48MB
          */
          SparkEnv.get.conf.get(config.REDUCER_MAX_SIZE_IN_FLIGHT) * 1024 * 1024,
          SparkEnv.get.conf.get(config.REDUCER_MAX_REQS_IN_FLIGHT),
          SparkEnv.get.conf.get(config.REDUCER_MAX_BLOCKS_IN_FLIGHT_PER_ADDRESS),
          SparkEnv.get.conf.get(config.MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM),
          SparkEnv.get.conf.get(config.SHUFFLE_MAX_ATTEMPTS_ON_NETTY_OOM),
          SparkEnv.get.conf.get(config.SHUFFLE_DETECT_CORRUPT),
          SparkEnv.get.conf.get(config.SHUFFLE_DETECT_CORRUPT_MEMORY),
          SparkEnv.get.conf.get(config.SHUFFLE_CHECKSUM_ENABLED),
          SparkEnv.get.conf.get(config.SHUFFLE_CHECKSUM_ALGORITHM),
          readMetrics,
          fetchContinuousBlocksInBatch).toCompletionIterator
    
        val serializerInstance = dep.serializer.newInstance()
    
        // Create a key/value iterator for each stream
        // 对每条流，创建一个键值对迭代器  【先反序列化流，再转成键值对形式】
        val recordIter = wrappedStreams.flatMap { case (blockId, wrappedStream) =>
          // Note: the asKeyValueIterator below wraps a key/value iterator inside of a
          // NextIterator. The NextIterator makes sure that close() is called on the
          // underlying InputStream when all records have been read.
          // 注意:下面的asKeyValueIterator将键/值迭代器封装在NextIterator中。NextIterator确保在读取所有记录时在底层的InputStream上调用close()。
          serializerInstance.deserializeStream(wrappedStream).asKeyValueIterator
        }
    
        // 【对 recordIter 的一次封装】   
        // Update the context task metrics for each record read.
        val metricIter = CompletionIterator[(Any, Any), Iterator[(Any, Any)]](
          recordIter.map { record =>
            readMetrics.incRecordsRead(1)
            record
          },
          context.taskMetrics().mergeShuffleReadMetrics())

        // 【对 recordIter 的再次一次封装，对迭代器提供了取消任务的功能】
        // An interruptible iterator must be used here in order to support task cancellation
        val interruptibleIter = new InterruptibleIterator[(Any, Any)](context, metricIter)
    
        // 【对 recordIter 的再次一次封装  （聚合操作）】
        val aggregatedIter: Iterator[Product2[K, C]] = if (dep.aggregator.isDefined) {
          if (dep.mapSideCombine) {
            // We are reading values that are already combined
            val combinedKeyValuesIterator = interruptibleIter.asInstanceOf[Iterator[(K, C)]]
            // 1.2.2节  将数据插入map里，过程中会应用 mergeCombiners  （分区内）
            dep.aggregator.get.combineCombinersByKey(combinedKeyValuesIterator, context)
          } else {
            // We don't know the value type, but also don't care -- the dependency *should*
            // have made sure its compatible w/ this aggregator, which will convert the value
            // type to the combined type C
            val keyValuesIterator = interruptibleIter.asInstanceOf[Iterator[(K, Nothing)]]
            // 1.2.2节 将数据插入map里，过程中会应用 mergeValue（分区内）
            dep.aggregator.get.combineValuesByKey(keyValuesIterator, context)
          }
        } 
        // 没有定义聚合器，就不执行任何聚合
        else {
          interruptibleIter.asInstanceOf[Iterator[Product2[K, C]]]
        }
    
        // 【聚合完了，开始排序】
        // Sort the output if there is a sort ordering defined.
        val resultIter: Iterator[Product2[K, C]] = dep.keyOrdering match {
          case Some(keyOrd: Ordering[K]) =>  // 使用 ExternalSorter 排序
            // Create an ExternalSorter to sort the data.
            val sorter =
              new ExternalSorter[K, C, C](context, ordering = Some(keyOrd), serializer = dep.serializer)
            // 1.2.3节 插入所有记录，更新相关任务指标，并返回一个完成迭代器，所有数据都被写入这个对象，由聚合器聚合
            sorter.insertAllAndUpdateMetrics(aggregatedIter)
          case None => // 没有指定keyOrdering，就直接输出 
            aggregatedIter
        }
    
        resultIter match {
          case _: InterruptibleIterator[Product2[K, C]] => resultIter
          case _ =>
            // Use another interruptible iterator here to support task cancellation as aggregator
            // or(and) sorter may have consumed previous interruptible iterator.
            new InterruptibleIterator[Product2[K, C]](context, resultIter)
        }
      }  
}
```

### 1.2.1 ShuffleBlockFetcherIterator

```scala
/**
 * An iterator that fetches multiple blocks. For local blocks, it fetches from the local block
 * manager. For remote blocks, it fetches them using the provided BlockTransferService.
 * 获取多个块的迭代器。
 * 对于本地块，就从本地块管理器获取。
 * 对于远程块，就使用提供的 BlockTransferService 获取。
 * 
 * This creates an iterator of (BlockID, InputStream) tuples so the caller can handle blocks
 * in a pipelined fashion as they are received.
 * 这会创建一个 (BlockID, InputStream) 二元组的迭代器，所以调用者可以在接收到块时以流水线方式处理块。
 * 
 * The implementation throttles the remote fetches so they don't exceed maxBytesInFlight to avoid
 * using too much memory.
 * 实现的瓶颈就是远程获取，所以不要超过 maxBytesInFlight, 以避免使用太多内存。
 *
 * @param context [[TaskContext]], used for metrics update
 *                TaskContext，用于度量更新
 * @param shuffleClient [[BlockStoreClient]] for fetching remote blocks
 *                      BlockStoreClient，用于获取远程块
 * @param blockManager [[BlockManager]] for reading local blocks
 *                     BlockManager，用于读取本地块
 * @param blocksByAddress list of blocks to fetch grouped by the [[BlockManagerId]].
 *                        For each block we also require two info: 1. the size (in bytes as a long
 *                        field) in order to throttle the memory usage; 2. the mapIndex for this
 *                        block, which indicate the index in the map stage.
 *                        Note that zero-sized blocks are already excluded, which happened in
 *                        [[org.apache.spark.MapOutputTracker.convertMapStatuses]].
 *                        获取的块的列表，按BlockManagerId分组。
 *                        对每个块，要求两个信息：1.大小，为压制内存使用 2.这个块的mapIndex, 它表示在map阶段的索引
 *                        大小为0的块早就被排除了，在 MapOutputTracker.convertMapStatuses 里排除的
 * @param mapOutputTracker [[MapOutputTracker]] for falling back to fetching the original blocks if
 *                         we fail to fetch shuffle chunks when push based shuffle is enabled.
 *                         MapOutputTracker。当启用基于推动的shuffle, 如果我们获取shuffle块失败时，就回滚到获取最初块的地方
 * @param streamWrapper A function to wrap the returned input stream.
 *                      封装了返回的输入流的函数
 * @param maxBytesInFlight max size (in bytes) of remote blocks to fetch at any given point.
 *                         获取远程块的最大大小
 * @param maxReqsInFlight max number of remote requests to fetch blocks at any given point.
 *                        获取远程块的请求的最大数量
 * @param maxBlocksInFlightPerAddress max number of shuffle blocks being fetched at any given point
 *                                    for a given remote host:port.
 *                                    从给定的远程服务器host:port，获取shuffle块的最大数量 
 * @param maxReqSizeShuffleToMem max size (in bytes) of a request that can be shuffled to memory.
 *                               可以被shuffle到内存的一次请求的最大大小
 * @param maxAttemptsOnNettyOOM The max number of a block could retry due to Netty OOM before
 *                              throwing the fetch failure.
 *                              在抛出获取失败之前，由于Netty OOM可以重试的最大块数。
 * @param detectCorrupt         whether to detect any corruption in fetched blocks.
 *                              是否可以检查获取块的损坏
 * @param checksumEnabled whether the shuffle checksum is enabled. When enabled, Spark will try to
 *                        diagnose the cause of the block corruption.
 * @param checksumAlgorithm the checksum algorithm that is used when calculating the checksum value
 *                         for the block data.
 * @param shuffleMetrics used to report shuffle metrics.
 * @param doBatchFetch fetch continuous shuffle blocks from same executor in batch if the server
 *                     side supports.
 *                     如果服务器端支持，从相同executor批量获取连续shuffle块 
 */
private[spark]
final class ShuffleBlockFetcherIterator(
    context: TaskContext,
    shuffleClient: BlockStoreClient,
    blockManager: BlockManager,
    mapOutputTracker: MapOutputTracker,
    blocksByAddress: Iterator[(BlockManagerId, Seq[(BlockId, Long, Int)])],
    streamWrapper: (BlockId, InputStream) => InputStream,
    maxBytesInFlight: Long,
    maxReqsInFlight: Int,
    maxBlocksInFlightPerAddress: Int,
    val maxReqSizeShuffleToMem: Long,
    maxAttemptsOnNettyOOM: Int,
    detectCorrupt: Boolean,
    detectCorruptUseExtraMemory: Boolean,
    checksumEnabled: Boolean,
    checksumAlgorithm: String,
    shuffleMetrics: ShuffleReadMetricsReporter,
    doBatchFetch: Boolean)
  extends Iterator[(BlockId, InputStream)] with DownloadFileManager with Logging {}
```

### 1.2.2 combineCombinersByKey combineCombinersByKey

Aggregator.scala

```scala
  def combineCombinersByKey(
      iter: Iterator[_ <: Product2[K, C]],
      context: TaskContext): Iterator[(K, C)] = {
    // 这里是 K, C, C  中间是聚合
    // 一个仅追加的map, 当没有足够的空间时，就会将有序内容溢写到磁盘
    val combiners = new ExternalAppendOnlyMap[K, C, C](identity, mergeCombiners, mergeCombiners)
    // 当一条新数据的插入map里，会判断、应用聚合函数，以更新map中的数据
    combiners.insertAll(iter)
    updateMetrics(context, combiners)
    combiners.iterator
  }
```

```scala
    def combineValuesByKey(
          iter: Iterator[_ <: Product2[K, V]],
          context: TaskContext): Iterator[(K, C)] = {
        // 这里是 K, V, C  中间是合并
        val combiners = new ExternalAppendOnlyMap[K, V, C](createCombiner, mergeValue, mergeCombiners)
        combiners.insertAll(iter)
        updateMetrics(context, combiners)
        combiners.iterator
    }
```

ExternalAppendOnlyMap.scala

```scala
/**
 * :: DeveloperApi ::
 * An append-only map that spills sorted content to disk when there is insufficient space for it
 * to grow.
 * 一个仅追加的map, 当没有足够的空间时，就会将有序内容溢写到磁盘
 * 
 * This map takes two passes over the data:
 * 这个 map 会两次传递数据:
 *   - 值被合并到combiners, 它被排序，可能会溢写到磁盘
 *   - 从磁盘读取Combiners, 再合并它们
 *   (1) Values are merged into combiners, which are sorted and spilled to disk as necessary
 *   (2) Combiners are read from disk and merged together
 *
 * The setting of the spill threshold faces the following trade-off: If the spill threshold is
 * too high, the in-memory map may occupy more memory than is available, resulting in OOM.
 * However, if the spill threshold is too low, we spill frequently and incur unnecessary disk
 * writes. This may lead to a performance regression compared to the normal case of using the
 * non-spilling AppendOnlyMap.
 */
@DeveloperApi
class ExternalAppendOnlyMap[K, V, C](
    createCombiner: V => C,
    mergeValue: (C, V) => C,
    mergeCombiners: (C, C) => C,
    serializer: Serializer = SparkEnv.get.serializer,
    blockManager: BlockManager = SparkEnv.get.blockManager,
    context: TaskContext = TaskContext.get(),
    serializerManager: SerializerManager = SparkEnv.get.serializerManager)
  extends Spillable[SizeTracker](context.taskMemoryManager())
  with Serializable
  with Logging
  with Iterable[(K, C)] {

  @volatile private[collection] var currentMap = new SizeTrackingAppendOnlyMap[K, C]
  
      /**
       * Insert the given iterator of keys and values into the map.
       * 将给定keys和values的迭代器插入到map中
       * 
       * When the underlying map needs to grow, check if the global pool of shuffle memory has
       * enough room for this to happen. If so, allocate the memory required to grow the map;
       * otherwise, spill the in-memory map to disk.
       * 当底层map需要增长时，会检查shuffle内存的全局池是否有足够的空间。
       * 如果没有，就分配map增长所需的内存，否则，就将内存中的map溢写到磁盘
       * 
       * The shuffle memory usage of the first trackMemoryThreshold entries is not tracked.
       */
      def insertAll(entries: Iterator[Product2[K, V]]): Unit = {
        if (currentMap == null) {
          throw new IllegalStateException(
            "Cannot insert new elements into a map after calling iterator")
        }
        // An update function for the map that we reuse across entries to avoid allocating
        // a new closure each time
        var curEntry: Product2[K, V] = null
        // 一个更新方法，如果存在就合并，否则就创建
        val update: (Boolean, C) => C = (hadVal, oldVal) => {
          if (hadVal) mergeValue(oldVal, curEntry._2) else createCombiner(curEntry._2)
        }
    
        while (entries.hasNext) {
          curEntry = entries.next()
          val estimatedSize = currentMap.estimateSize()
          if (estimatedSize > _peakMemoryUsedBytes) {
            _peakMemoryUsedBytes = estimatedSize
          }
          // 当来了一条数据时，先判断map里的空间够不够，要不要溢写，如果溢写了，就创建一个新的 map
          if (maybeSpill(currentMap, estimatedSize)) {
            currentMap = new SizeTrackingAppendOnlyMap[K, C]
          }
          // 将新数据放入map里，并应用前面定义的update方法
          currentMap.changeValue(curEntry._1, update)
          addElementsRead()
        }
      }
}

```

### 1.2.3 sorter.insertAllAndUpdateMetrics()

ExternalSorter.scala

```scala
  /**
   * Insert all records, updates related task metrics, and return a completion iterator
   * over all the data written to this object, aggregated by our aggregator.
   * On task completion (success, failure, or cancellation), it releases resources by
   * calling `stop()`.
   * 插入所有记录，更新相关任务指标，并返回一个完成迭代器，所有数据都被写入这个对象，由聚合器聚合。
   * 在任务完成时(成功、失败或取消)，它通过调用stop()来释放资源。
   */
  def insertAllAndUpdateMetrics(records: Iterator[Product2[K, V]]): Iterator[Product2[K, C]] = {
    insertAll(records)
    context.taskMetrics().incMemoryBytesSpilled(memoryBytesSpilled)
    context.taskMetrics().incDiskBytesSpilled(diskBytesSpilled)
    context.taskMetrics().incPeakExecutionMemory(peakMemoryUsedBytes)
    // Use completion callback to stop sorter if task was finished/cancelled.
    context.addTaskCompletionListener[Unit](_ => stop())
    CompletionIterator[Product2[K, C], Iterator[Product2[K, C]]](iterator, stop())
  }
```