
[TOC]

DAGScheduler 负责阶段的划分调度、任务的切分

以 WordCount 中的核心代码为例

```scala
   val lines: RDD[String] = sc.textFile("datas")
   val words: RDD[String] = lines.flatMap(_.split(" "))
   val wordToOne = words.map(word=>(word,1))
   val wordToSum: RDD[(String, Int)] = wordToOne.reduceByKey(_+_)
   val array: Array[(String, Int)] = wordToSum.collect()
```

这里从作业的 action 算子开始分析 （要注意参数的流转，比如 rdd 和 partitions）

RDD.scala

```scala
def collect(): Array[T] = withScope {
    // this就是 wordToSum
    val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray)
    Array.concat(results: _*)
  }
```

SparkContext.scala

```scala
private[spark] def dagScheduler: DAGScheduler = _dagScheduler

def runJob[T, U: ClassTag](rdd: RDD[T], func: Iterator[T] => U): Array[U] = {
  runJob(rdd, func, 0 until rdd.partitions.length)
}

def runJob[T, U: ClassTag](
      rdd: RDD[T], 
      func: (TaskContext, Iterator[T]) => U,
      partitions: Seq[Int],
      resultHandler: (Int, U) => Unit): Unit = {
    //....
    // 在 SparkContext 中，调用 DAGScheduler 的 runJob 方法
    // rdd 就是 wordToSum
    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)
    //....
  }
```

DAGScheduler.scala

```scala
  // Run an action job on the given RDD and pass all the results to the resultHandler function as they arrive.
  def runJob[T, U](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) => U,
      partitions: Seq[Int],
      callSite: CallSite,
      resultHandler: (Int, U) => Unit,
      properties: Properties): Unit = {
    // 将一个action job提交给 scheduler
    // rdd 就是 wordToSum
    val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)
    //...
  }

  private[spark] val eventProcessLoop = new DAGSchedulerEventProcessLoop(this)

   // Submit an action job to the scheduler.
   def submitJob[T, U](): JobWaiter[U] = { 
     // .... 
     // post: 把 JobSubmitted 事件放到事件队列里，事件线程就可以随后处理
   eventProcessLoop.post(JobSubmitted(
        jobId, rdd, func2, partitions.toArray, callSite, waiter,  // rdd 就是 wordToSum
        Utils.cloneProperties(properties)))
    }
```

EventLoop.scala

```scala
/**
 * An event loop to receive events from the caller and process all events in the event thread. It
 * will start an exclusive event thread to process all events.
 *
 */
private[spark] abstract class EventLoop[E](name: String) extends Logging {

  private val stopped = new AtomicBoolean(false)
  private val eventQueue: BlockingQueue[E] = new LinkedBlockingDeque[E]()
  
  // Put the event into the event queue. The event thread will process it later.
  def post(event: E): Unit = {
    // 默认是false，往里执行
    if (!stopped.get) {
      if (eventThread.isAlive) {
        // 把事件放到事件队列里，事件线程就可以随后处理
        eventQueue.put(event)
      } else {
        onError(new IllegalStateException(s"$name has already been stopped accidentally."))
      }
    }
  }

  private[spark] val eventThread = new Thread(name) {
    setDaemon(true)

    // 当线程运行时，就开始处理前面放到事件队列里的事件
    override def run(): Unit = {
      //...
      // 取出来事件
      val event = eventQueue.take()
      onReceive(event)
    }
  }

  /**
   * Invoked in the event thread when polling events from the event queue.
   * 当从事件队列中取出事件后，在事件线程中调用它
   */
  protected def onReceive(event: E): Unit
}
```

下面查看 EventLoop 子类 DAGSchedulerEventProcessLoop 中的 onReceive 方法

DAGScheduler.scala

```scala
private[scheduler] class DAGSchedulerEventProcessLoop(dagScheduler: DAGScheduler)
  extends EventLoop[DAGSchedulerEvent]("dag-scheduler-event-loop") with Logging {
  /**
   * The main event loop of the DAG scheduler.
   */
  override def onReceive(event: DAGSchedulerEvent): Unit = {
    val timerContext = timer.time()
    try {
      doOnReceive(event)
    } finally {
      timerContext.stop()
    }
  }

  private def doOnReceive(event: DAGSchedulerEvent): Unit = { 
    // 前面往事件队列中放入了 JobSubmitted 事件，这里就匹配上了
    event match {
      // rdd 就是 wordToSum
        case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =>
          // 这里就处理事件
          dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)
        //....
      }
  }
}
```
DAGScheduler.scala

```scala
// 在这里进行阶段的划分
private[scheduler] def handleJobSubmitted(jobId: Int,
      finalRDD: RDD[_],
      func: (TaskContext, Iterator[_]) => _,
      partitions: Array[Int],
      callSite: CallSite,
      listener: JobListener,
      properties: Properties): Unit = {
    var finalStage: ResultStage = null
    try {
      // New stage creation may throw an exception if, for example, jobs are run on a
      // HadoopRDD whose underlying HDFS files have been deleted.
      // 创建一个和提供的 jobId 相关联的 ResultStage
      // rdd 就是 wordToSum
      finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)
    } catch {
      //....
    }
    // ...
    submitStage(finalStage)
  }
```
DAGScheduler.scala

```scala
/**
   * Create a ResultStage associated with the provided jobId.
   * 创建一个和提供的 jobId 相关联的 ResultStage
   */
  private def createResultStage(
      rdd: RDD[_],  // 就是 wordToSum  reducebykey产生的 shuffleRDD
      func: (TaskContext, Iterator[_]) => _,
      partitions: Array[Int],
      jobId: Int,
      callSite: CallSite): ResultStage = {
    // 1.1 节 shuffleDeps包含了wordToSum这个rdd的ShuffleDependency 
    val (shuffleDeps, resourceProfiles) = getShuffleDependenciesAndResourceProfiles(rdd)
    val resourceProfile = mergeResourceProfilesForStage(resourceProfiles)
    checkBarrierStageWithDynamicAllocation(rdd)
    checkBarrierStageWithNumSlots(rdd, resourceProfile)
    checkBarrierStageWithRDDChainPattern(rdd, partitions.toSet.size)
    // 1.2 节   parents 包含了 基于 调用reduceByKey的rdd(wordToOne) 创建的 ShuffleMapStage 
    val parents = getOrCreateParentStages(shuffleDeps, jobId)
    val id = nextStageId.getAndIncrement()
    // 创建ResultStage对象
    val stage = new ResultStage(id, rdd, func, partitions, parents, jobId,
      callSite, resourceProfile.id)
    stageIdToStage(id) = stage
    updateJobIdStageIdMaps(jobId, stage)
    stage
  }
```

## 1.1 getShuffleDependenciesAndResourceProfiles()

DAGScheduler.scala

```scala
/**
   * Returns shuffle dependencies that are immediate parents of the given RDD and the
   * ResourceProfiles associated with the RDDs for this stage.
   * 返回给定 RDD 的直接父shuffle依赖 和 与这个阶段的 RDDs 相关联的ResourceProfiles 
   * 
   * This function will not return more distant ancestors for shuffle dependencies. For example,
   * if C has a shuffle dependency on B which has a shuffle dependency on A:
   *
   * A <-- B <-- C    【只会返回 B <-- C 的依赖】
   *
   * calling this function with rdd C will only return the B <-- C dependency.
   *
   * This function is scheduler-visible for the purpose of unit testing.
   */
  private[scheduler] def getShuffleDependenciesAndResourceProfiles(
      // rdd 就是 wordToSum                                                        
      rdd: RDD[_]): (HashSet[ShuffleDependency[_, _, _]], HashSet[ResourceProfile]) = {
    val parents = new HashSet[ShuffleDependency[_, _, _]]
    val resourceProfiles = new HashSet[ResourceProfile]
    val visited = new HashSet[RDD[_]]
    val waitingForVisit = new ListBuffer[RDD[_]]
    // 先把传入的这个 rdd 放到waitingForVisit里
    waitingForVisit += rdd
    while (waitingForVisit.nonEmpty) {
      // 再取出来传入的这个 rdd，并从waitingForVisit里删除
      val toVisit = waitingForVisit.remove(0)
      // 没有被访问过
      if (!visited(toVisit)) {
        // 添加到 visited 这个hashset里
        visited += toVisit
        Option(toVisit.getResourceProfile).foreach(resourceProfiles += _)
        // 1.1.1节 toVisit.dependencies 返回的是 ShuffleDependency
        toVisit.dependencies.foreach {
          case shuffleDep: ShuffleDependency[_, _, _] =>
            // 把 ShuffleDependency 添加到 ShuffleDependency parents这个hashset里
            parents += shuffleDep
          case dependency =>
            waitingForVisit.prepend(dependency.rdd)
        }
      }
    }
    // 返回含有ShuffleDependency依赖的hashset里
    (parents, resourceProfiles)
  }
```
### 1.1.1 toVisit.dependencies

RDD.scala

```scala

  /** An Option holding our checkpoint RDD, if we are checkpointed */
    // 持有checkpoint RDD
  private def checkpointRDD: Option[CheckpointRDD[T]] = checkpointData.flatMap(_.checkpointRDD)

  /**
   * Get the list of dependencies of this RDD, taking into account whether the
   * RDD is checkpointed or not.
   */
  final def dependencies: Seq[Dependency[_]] = {
    // 以上面的wordcount为例，没有做checkpoint，所以，这里返回的是 ShuffleDependency (wordToSum)
    checkpointRDD
      // 基于checkpoint RDD，创建一个 OneToOneDependency 依赖的列表
      .map(r => List(new OneToOneDependency(r)))
      // 如果前面创建了窄依赖的列表就返回，否则就执行里面的默认方法
      .getOrElse {
          if (dependencies_ == null) {
            stateLock.synchronized {
              if (dependencies_ == null) {
                dependencies_ = getDependencies
              }
            }
          }
          dependencies_
    }
  }

  /**
   * Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only
   * be called once, so it is safe to implement a time-consuming computation in it.
   * 由子类实现，为了返回这个rdd如何依赖父rdds
   */
  // 前面传入的rdd是reducebykey产生的shuffleRDD,就是wordToSum，所以这里的依赖就是ShuffleDependency
  protected def getDependencies: Seq[Dependency[_]] = deps
```

## 1.2 getOrCreateParentStages()

DAGScheduler.scala

```scala
    /**
     * Get or create the list of parent stages for the given shuffle dependencies. The new
     * Stages will be created with the provided firstJobId.
     * 获得或创建给定的 ShuffleDependency 的父stages的列表 
     */
    private def getOrCreateParentStages(shuffleDeps: HashSet[ShuffleDependency[_, _, _]],
                                        firstJobId: Int): List[Stage] = {
      shuffleDeps.map { shuffleDep =>
        /*
        * 在第一节了解SparkContext的内部对象时，知道：
        *  有两种类型的阶段：
        *     （1）ResultStage: 执行action操作的最后的阶段 
        *     （2）ShuffleMapStage: 在一次shuffle中，写入 map 输出文件
        * */
         
        getOrCreateShuffleMapStage(shuffleDep, firstJobId)
      }.toList // 将基于 调用reduceByKey的rdd(wordToOne) 创建的 ShuffleMapStage 转成list返回
    }
```

DAGScheduler.scala

```scala
    /**
     * Mapping from shuffle dependency ID to the ShuffleMapStage that will generate the data for
     * that dependency. Only includes stages that are part of currently running job (when the job(s)
     * that require the shuffle stage complete, the mapping will be removed, and the only record of
     * the shuffle data will be in the MapOutputTracker).
     * key 是 ShuffleDependency的id, value 是 ShuffleMapStage
     */
    private[scheduler] val shuffleIdToMapStage = new HashMap[Int, ShuffleMapStage]

 /**
   * Gets a shuffle map stage if one exists in shuffleIdToMapStage. Otherwise, if the
   * shuffle map stage doesn't already exist, this method will create the shuffle map stage in
   * addition to any missing ancestor shuffle map stages.
   * 如果在 shuffleIdToMapStage 集合中存在一个 ShuffleMapStage, 那么就获取它。
   * 如果不里面不存在，这个方法就创建ShuffleMapStage
   */
 // 返回 基于 调用reduceByKey的rdd(wordToOne) 创建 ShuffleMapStage
  private def getOrCreateShuffleMapStage(
      shuffleDep: ShuffleDependency[_, _, _],
      firstJobId: Int): ShuffleMapStage = {
    shuffleIdToMapStage.get(shuffleDep.shuffleId) match {
      // 如果在 shuffleIdToMapStage 集合中存在一个 ShuffleMapStage, 那么就获取它。
      case Some(stage) =>
        stage
      // 如果不里面不存在，这个方法就创建ShuffleMapStage
      case None =>
        // 查找尚未在 shuffleIdToMapStage 中注册的祖先ShuffleDependency
        // 返回包含未注册的祖先ShuffleDependency的ListBuffer
        // Create stages for all missing ancestor shuffle dependencies.
        getMissingAncestorShuffleDependencies(shuffleDep.rdd).foreach { dep =>
          // Even though getMissingAncestorShuffleDependencies only returns shuffle dependencies
          // that were not already in shuffleIdToMapStage, it's possible that by the time we
          // get to a particular dependency in the foreach loop, it's been added to
          // shuffleIdToMapStage by the stage creation process for an earlier dependency. See
          // SPARK-13902 for more information.
          if (!shuffleIdToMapStage.contains(dep.shuffleId)) {
            createShuffleMapStage(dep, firstJobId)
          }
        }
        // 在此次分析情况下，shuffleIdToMapStage里是不包含传入的ShuffleDependency
        // 这里 shuffleDep：wordToSum这个rdd的ShuffleDependency 
        // Finally, create a stage for the given shuffle dependency.
        createShuffleMapStage(shuffleDep, firstJobId)
    }
  }
```
DAGScheduler.scala

```scala
/**
   * Creates a ShuffleMapStage that generates the given shuffle dependency's partitions. If a
   * previously run stage generated the same shuffle data, this function will copy the output
   * locations that are still available from the previous shuffle to avoid unnecessarily
   * regenerating data.
   */
  // 返回 基于 调用reduceByKey的rdd(wordToOne) 创建 ShuffleMapStage
  def createShuffleMapStage[K, V, C](
      shuffleDep: ShuffleDependency[K, V, C], jobId: Int): ShuffleMapStage = {
    // 这个rdd是调用reduceByKey的rdd(wordToOne)
    val rdd = shuffleDep.rdd
    // 这个 wordToOne RDD前面没有shuffle了，所以shuffleDeps是空的集合  （1.1节分析过这个函数）
    val (shuffleDeps, resourceProfiles) = getShuffleDependenciesAndResourceProfiles(rdd)
    val resourceProfile = mergeResourceProfilesForStage(resourceProfiles)
    checkBarrierStageWithDynamicAllocation(rdd)
    checkBarrierStageWithNumSlots(rdd, resourceProfile)
    checkBarrierStageWithRDDChainPattern(rdd, rdd.getNumPartitions)
    val numTasks = rdd.partitions.length
    // 返回一个空集合
    val parents = getOrCreateParentStages(shuffleDeps, jobId)
    val id = nextStageId.getAndIncrement()
    // 基于 调用reduceByKey的rdd(wordToOne) 创建 ShuffleMapStage
    val stage = new ShuffleMapStage(
      id, rdd, numTasks, parents, jobId, rdd.creationSite, shuffleDep, mapOutputTracker,
      resourceProfile.id)

    stageIdToStage(id) = stage
    shuffleIdToMapStage(shuffleDep.shuffleId) = stage
    updateJobIdStageIdMaps(jobId, stage)

    if (!mapOutputTracker.containsShuffle(shuffleDep.shuffleId)) {
      // Kind of ugly: need to register RDDs with the cache and map output tracker here
      // since we can't do it in the RDD constructor because # of partitions is unknown
      logInfo(s"Registering RDD ${rdd.id} (${rdd.getCreationSite}) as input to " +
        s"shuffle ${shuffleDep.shuffleId}")
      mapOutputTracker.registerShuffle(shuffleDep.shuffleId, rdd.partitions.length,
        shuffleDep.partitioner.numPartitions)
    }
    stage
  }
```

# 2 Stage类

Stage.scala

```scala
/**
 * A stage is a set of parallel tasks all computing the same function that need to run as part
 * of a Spark job, where all the tasks have the same shuffle dependencies. Each DAG of tasks run
 * by the scheduler is split up into stages at the boundaries where shuffle occurs, and then the
 * DAGScheduler runs these stages in topological order.
 * 一个阶段是一个并行任务的集合，这些任务运行相同函数，这个函数作为 spark job 的一部分运行，
 * 所有的任务都有相同的 shuffle 依赖。
 * 由调度器运行的任务的DAG 被划分成多个阶段，在shuffle出现的位置划分，然后 DAGScheduler 以拓扑顺序运行这些阶段。
 *
 * Each Stage can either be a shuffle map stage, in which case its tasks' results are input for
 * other stage(s), or a result stage, in which case its tasks directly compute a Spark action
 * (e.g. count(), save(), etc) by running a function on an RDD. For shuffle map stages, we also
 * track the nodes that each output partition is on.
 * 每个阶段要么是 shuffle map stage, 在这种情况下，它的任务的结果是其他阶段任务的输入，
 *       要么是 result stage, 在这种情况下，它的任务通过在 RDD 上运行一个函数来直接计算 Spark action 算子（如count save）
 * 对于 shuffle map stage, 我们也追踪每个输出分区所在的节点
 *
 * Each Stage also has a firstJobId, identifying the job that first submitted the stage.  When FIFO
 * scheduling is used, this allows Stages from earlier jobs to be computed first or recovered
 * faster on failure.
 * 每个阶段也有一个 firstJobId, 它识别第一个提交到这个阶段的 job.
 * 当使用 FIFO 调度器时，这就允许 Stages 从最早的jobs开始计算，或者在失败时快速恢复。
 *
 * Finally, a single stage can be re-executed in multiple attempts due to fault recovery. In that
 * case, the Stage object will track multiple StageInfo objects to pass to listeners or the web UI.
 * The latest one will be accessible through latestInfo.
 * 最后，一个阶段在故障恢复是，可以多次尝试执行。
 * 在那种情况下，Stage对象可以追踪多个 StageInfo 对象，来传递给监听器或web ui
 * 最后一个 StageInfo 对象将通过 latestInfo 访问。
 *  
 * @param id Unique stage ID
 * 唯一的阶段id
 * @param rdd RDD that this stage runs on: for a shuffle map stage, it's the RDD we run map tasks
 *   on, while for a result stage, it's the target RDD that we ran an action on
 * 这个阶段运行在的RDD: 对于一个shuffle map stage, 它是运行 map 任务的RDD;
 *                   对于一个result stage, 它是运行action算子的RDD
 * @param numTasks Total number of tasks in stage; result stages in particular may not need to
 *   compute all partitions, e.g. for first(), lookup(), and take().
 * 阶段中任务的总数量；特别是 result stages, 它不需要计算所有的分区，比如  first lookup take
 * @param parents List of stages that this stage depends on (through shuffle dependencies).
 * 这个阶段所依赖的阶段的列表 （通过shuffle依赖）            
 * @param firstJobId ID of the first job this stage was part of, for FIFO scheduling.
 * 这个阶段所属于的第一个job的id，对于FIFO调度                  
 * @param callSite Location in the user program associated with this stage: either where the target
 *   RDD was created, for a shuffle map stage, or where the action for a result stage was called.
 * 用户程序中，和这个阶段相关联的位置：要么是创建目标RDD的位置（shuffle map stage），要么是调用action算子的位置（result stage） 
 */
private[scheduler] abstract class Stage(
    val id: Int,  
    val rdd: RDD[_],
    val numTasks: Int,
    val parents: List[Stage],
    val firstJobId: Int,
    val callSite: CallSite,
    val resourceProfileId: Int)
  extends Logging {}
```

## 2.1 ShuffleMapStage类

ShuffleMapStage.scala

```scala
/**
 * ShuffleMapStages are intermediate stages in the execution DAG that produce data for a shuffle.
 * They occur right before each shuffle operation, and might contain multiple pipelined operations
 * before that (e.g. map and filter). When executed, they save map output files that can later be
 * fetched by reduce tasks. The `shuffleDep` field describes the shuffle each stage is part of,
 * and variables like `outputLocs` and `numAvailableOutputs` track how many map outputs are ready.
 * ShuffleMapStages 在执行 DAG 中是中间阶段。
 * 它们正好在每次 shuffle 操作前出现，可能会包含多个管道化的操作（比如 map filter）。
 * 当 ShuffleMapStages 执行完成后，保存 map 输出文件，这个文件可以稍后被 reduce tasks 获取。
 * `shuffleDep` 字段描述了每个 stage 的 shuffle, 
 * 像 `outputLocs` and `numAvailableOutputs` 变量追踪有多少 map 输出准备好了
 * 
 * ShuffleMapStages can also be submitted independently as jobs with DAGScheduler.submitMapStage.
 * For such stages, the ActiveJobs that submitted them are tracked in `mapStageJobs`. Note that
 * there can be multiple ActiveJobs trying to compute the same shuffle map stage.
 * ShuffleMapStages 也可以使用 `DAGScheduler.submitMapStage` 独立地作为 jobs 提交。
 * 对于这种阶段，提交它们的 ActiveJobs 会在 `mapStageJobs` 中被跟踪。
 * 请注意，可能有多个 ActiveJobs 尝试计算相同的 shuffle map stage.
 */
private[spark] class ShuffleMapStage(
    id: Int,
    rdd: RDD[_],
    numTasks: Int,
    parents: List[Stage],
    firstJobId: Int,
    callSite: CallSite,
    val shuffleDep: ShuffleDependency[_, _, _],
    mapOutputTrackerMaster: MapOutputTrackerMaster,
    resourceProfileId: Int)
  extends Stage(id, rdd, numTasks, parents, firstJobId, callSite, resourceProfileId) {}
```

## 2.2 ResultStage类

ResultStage.scala

```scala
/**
 * ResultStages apply a function on some partitions of an RDD to compute the result of an action.
 * The ResultStage object captures the function to execute, `func`, which will be applied to each
 * partition, and the set of partition IDs, `partitions`. Some stages may not run on all partitions
 * of the RDD, for actions like first() and lookup().
 * ResultStages 在一个 RDD 的一些分区上应用一个函数，来计算一个 action 算子的结果。
 * ResultStages 对象捕获要执行的函数(`func`)，函数将被应用在每个分区；和分区ids的集合(`partitions`)
 * 一些阶段可能不会在 RDD 的所有分区上运行，比如 first lookup
 */
private[spark] class ResultStage(
    id: Int,
    rdd: RDD[_],
    val func: (TaskContext, Iterator[_]) => _,
    val partitions: Array[Int],
    parents: List[Stage],
    firstJobId: Int,
    callSite: CallSite,
    resourceProfileId: Int)
  extends Stage(id, rdd, partitions.length, parents, firstJobId, callSite, resourceProfileId){}
```
