
在 `04_任务的切分.md` 中分析到，`submitMissingTasks` 方法中，会根据分区的数量创建对应数量的任务，接下来就需要调度这些创建的任务了。

DAGScheduler.scala

```scala
  private def submitMissingTasks(stage: Stage, jobId: Int): Unit = {
    //....
    val tasks: Seq[Task[_]] = try {}
    //....

    if (tasks.nonEmpty) {
      logInfo(s"Submitting ${tasks.size} missing tasks from $stage (${stage.rdd}) (first 15 " +
        s"tasks are for partitions ${tasks.take(15).map(_.partitionId)})")
      // 提交一个任务序列来运行  
      // 先将任务封装成TaskSet （TaskSet:一起提交给低等级的TaskScheduler的一组任务，通常表示某个特定阶段缺失的（待计算的）分区）
      taskScheduler.submitTasks(new TaskSet(
        tasks.toArray, stage.id, stage.latestInfo.attemptNumber, jobId, properties,
        stage.resourceProfileId))
    } else {
      //....
      submitWaitingChildStages(stage)
    }
  }
```

TaskScheduler.scala

```scala
// Submit a sequence of tasks to run.
  def submitTasks(taskSet: TaskSet): Unit
```

查看其实现类 TaskSchedulerImpl

TaskSchedulerImpl.scala

```scala
private[spark] class TaskSchedulerImpl(
    val sc: SparkContext,
    val maxTaskFailures: Int,
    isLocal: Boolean = false,
    clock: Clock = new SystemClock)
  extends TaskScheduler with Logging {
  
  private val starvationTimer = new Timer("task-starvation-timer", true)

  override def submitTasks(taskSet: TaskSet): Unit = {
    // 取到数组形式的任务集合
    val tasks = taskSet.tasks
    logInfo("Adding task set " + taskSet.id + " with " + tasks.length + " tasks "
      + "resource profile " + taskSet.resourceProfileId)
    // 把任务放进这个任务池里 ⬇
    this.synchronized {
      // 1.1节 创建一个TaskSetManager,  TaskSetManager会调度一个 TaskSet 中的任务，
      val manager = createTaskSetManager(taskSet, maxTaskFailures)
      // 这个 taskSet 所属的 stage
      val stage = taskSet.stageId
      // [这个 taskSet 所属的 stage, [stageAttemptId, TaskSetManager]]
      val stageTaskSets =
        taskSetsByStageIdAndAttempt.getOrElseUpdate(stage, new HashMap[Int, TaskSetManager])

      // Mark all the existing TaskSetManagers of this stage as zombie, as we are adding a new one.
      // This is necessary to handle a corner case. Let's say a stage has 10 partitions and has 2
      // TaskSetManagers: TSM1(zombie) and TSM2(active). TSM1 has a running task for partition 10
      // and it completes. TSM2 finishes tasks for partition 1-9, and thinks he is still active
      // because partition 10 is not completed yet. However, DAGScheduler gets task completion
      // events for all the 10 partitions and thinks the stage is finished. If it's a shuffle stage
      // and somehow it has missing map outputs, then DAGScheduler will resubmit it and create a
      // TSM3 for it. As a stage can't have more than one active task set managers, we must mark
      // TSM2 as zombie (it actually is).
      stageTaskSets.foreach { case (_, ts) =>
        // 对于这个 TaskSetManager，一旦它没有任务要启动，就进入 Zombie 状态
        ts.isZombie = true
      }
      stageTaskSets(taskSet.stageAttemptId) = manager
      // 1.2节  1.3节  根据不同的调度策略（默认是FIFO），创建不同的schedulableBuilder
      // 把封装了taskSet的TaskSetManager放入一个可调度的pool里  （可以把这个 pool 理解成任务池） （把任务放进这个 pool 里）
      schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)
      // 默认都是false   【？？】
      if (!isLocal && !hasReceivedTask) {
        // 调度指定的任务（new出来的TimerTask对象），在指定延迟之后开始以固定速率重复执行。随后的执行以大约有规律的间隔进行，间隔由指定的时间段分隔。
        starvationTimer.scheduleAtFixedRate(new TimerTask() {
          override def run(): Unit = {
            if (!hasLaunchedTask) { // 默认false
              logWarning("Initial job has not accepted any resources; " +
                "check your cluster UI to ensure that workers are registered " +
                "and have sufficient resources")
            } else {
              this.cancel() // 取消定时任务
            }
          }
        }, STARVATION_TIMEOUT_MS, STARVATION_TIMEOUT_MS) // 第一个是延迟执行时间  第二个是执行间隔
      }
      hasReceivedTask = true
    }
    // 1.4节  从 pool(任务池) 里取出来任务，对任务排个执行顺序，向 executors 发送任务，并通知 executors 启动任务
    backend.reviveOffers()
  }
}
```

## 1.1 createTaskSetManager()

TaskSchedulerImpl.scala

```scala
private[scheduler] def createTaskSetManager(
      taskSet: TaskSet,
      maxTaskFailures: Int): TaskSetManager = {
    // this就是 TaskSchedulerImpl对象
    new TaskSetManager(this, taskSet, maxTaskFailures, healthTrackerOpt, clock)
  }
```

TaskSetManager.scala

```scala
/**
 * Schedules the tasks within a single TaskSet in the TaskSchedulerImpl. This class keeps track of
 * each task, retries tasks if they fail (up to a limited number of times), and
 * handles locality-aware scheduling for this TaskSet via delay scheduling. The main interfaces
 * to it are resourceOffer, which asks the TaskSet whether it wants to run a task on one node,
 * and handleSuccessfulTask/handleFailedTask, which tells it that one of its tasks changed state
 *  (e.g. finished/failed).
 *  在 TaskSchedulerImpl 中，调度一个 TaskSet 中的任务。
 *  这个类追踪每个任务、如果任务失败（超过限制的次数）就重试任务、通过延迟调度为这个 TaskSet 处理基于位置感知的调度。
 *  实现它的主要的接口就是 resourceOffer 方法，它会询问 TaskSet 是否想要在一个节点上运行一个任务，
 *      以及 handlessuccessfultask /handleFailedTask，它告诉 TaskSet, 它的一个任务改变了状态（例如完成、失败）
 *
 * THREADING: This class is designed to only be called from code with a lock on the
 * TaskScheduler (e.g. its event handlers). It should not be called from other threads.
 *
 * @param sched           the TaskSchedulerImpl associated with the TaskSetManager
 *                        和这个 TaskSetManager 相关的 TaskSchedulerImpl
 * @param taskSet         the TaskSet to manage scheduling for
 *                        管理调度的 TaskSet
 * @param maxTaskFailures if any particular task fails this number of times, the entire
 *                        task set will be aborted
 *                        如果任意特定的任务失败指定次数，整个任务集就被夭折
 */
private[spark] class TaskSetManager(   // 是一个 Schedulable 的实体
    sched: TaskSchedulerImpl,
    val taskSet: TaskSet,
    val maxTaskFailures: Int,
    healthTracker: Option[HealthTracker] = None,
    clock: Clock = new SystemClock()) extends Schedulable with Logging {
  
  // True once no more tasks should be launched for this task set manager. TaskSetManagers enter
  // the zombie state once at least one attempt of each task has completed successfully, or if the
  // task set is aborted (for example, because it was killed).  TaskSetManagers remain in the zombie
  // state until all tasks have finished running; we keep TaskSetManagers that are in the zombie
  // state in order to continue to track and account for the running tasks.
  // TODO: We should kill any running task attempts when the task set manager becomes a zombie.
  /*
     对于这个 TaskSetManager，一旦它没有任务要启动，就是返回 True.
     一旦每个任务至少有一次尝试是成功完成的，或任务集夭折了（例如被kill），TaskSetManager 就进入 zombie 状态。
     TaskSetManager 保持在 zombie 状态，知道所有任务运行完成；
     我们保持 TaskSetManager 在 zombie 状态，是为了继续追踪和 account for 正在运行的任务。
   */
  private[scheduler] var isZombie = false
}
```

## 1.2 schedulableBuilder.addTaskSetManager()

```scala
/**
 * An interface to build Schedulable tree   构建可调度的树的接口
 * buildPools: build the tree nodes(pools)  构建树节点
 * addTaskSetManager: build the leaf nodes(TaskSetManagers)  构建叶节点
 */
private[spark] trait SchedulableBuilder {
  // 1.2.1节 
  def rootPool: Pool

  def buildPools(): Unit

  // 1.2.2节 
  def addTaskSetManager(manager: Schedulable, properties: Properties): Unit
}

private[spark] class FIFOSchedulableBuilder(val rootPool: Pool)
  extends SchedulableBuilder with Logging {

  override def buildPools(): Unit = {
    // nothing
  }

  override def addTaskSetManager(manager: Schedulable, properties: Properties): Unit = {
    // 往 Pool 里添加 TaskSetManager 
    rootPool.addSchedulable(manager)
  }
}
```

### 1.2.1 Pool

Schedulable 表示一个可调度实体，有两种类型的实体 Pool 和 TaskSetManager （实现了Schedulable接口）

- Pool 里可以存放 Pool 和 TaskSetManager
- TaskSetManager 基于 TaskSet 封装，可以调度 TaskSet 中的任务

```scala
/**
 * A Schedulable entity that represents collection of Pools or TaskSetManagers
 * 表示 Pools 或 TaskSetManagers 的集合的 Schedulable 实体
 */
private[spark] class Pool(
    val poolName: String,
    val schedulingMode: SchedulingMode, // FAIR, FIFO, NONE 三种策略
    initMinShare: Int,
    initWeight: Int)
  extends Schedulable with Logging {}
```

### 1.2.2 Schedulable

```scala
/**
 * An interface for schedulable entities.
 * there are two type of Schedulable entities(Pools and TaskSetManagers)
 * 可调度实体的接口。
 * 有两种类型的实体: Pool  TaskSetManager
 */
private[spark] trait Schedulable {
  var parent: Pool
  // child queues  子队列
  def schedulableQueue: ConcurrentLinkedQueue[Schedulable]
  def schedulingMode: SchedulingMode  // FAIR, FIFO, NONE 三种策略
  def weight: Int
  def minShare: Int
  def runningTasks: Int
  def priority: Int
  def stageId: Int
  def name: String

  def isSchedulable: Boolean
  def addSchedulable(schedulable: Schedulable): Unit
  def removeSchedulable(schedulable: Schedulable): Unit
  def getSchedulableByName(name: String): Schedulable
  def executorLost(executorId: String, host: String, reason: ExecutorLossReason): Unit
  def executorDecommission(executorId: String): Unit
  def checkSpeculatableTasks(minTimeToSpeculation: Long): Boolean
  def getSortedTaskSetQueue: ArrayBuffer[TaskSetManager]
}
```

```scala
/**
 *  "FAIR" and "FIFO" determines which policy is used
 *    to order tasks amongst a Schedulable's sub-queues
 *  "NONE" is used when the a Schedulable has no sub-queues.
 *  "FAIR" 和 "FIFO" 决定使用哪个策略在 Schedulable 的子队列中排序任务
 *  当 Schedulable 没有子队列时，使用 "NONE"
 */
object SchedulingMode extends Enumeration {

  type SchedulingMode = Value
  val FAIR, FIFO, NONE = Value
}
```

## 1.3 在 TaskSchedulerImpl 中为 schedulableBuilder 赋值

```scala
val rootPool: Pool = new Pool("", schedulingMode, 0, 0)

private var schedulableBuilder: SchedulableBuilder = null

// default scheduler is FIFO
private val schedulingModeConf = conf.get(SCHEDULER_MODE)

val schedulingMode: SchedulingMode =
  try {
    SchedulingMode.withName(schedulingModeConf)
  } catch {
    case e: java.util.NoSuchElementException =>
      throw SparkCoreErrors.unrecognizedSchedulerModePropertyError(SCHEDULER_MODE_PROPERTY,
        schedulingModeConf)
  }

// 根据不同的调度策略（默认是FIFO），创建不同的schedulableBuilder
def initialize(backend: SchedulerBackend): Unit = {
    this.backend = backend
    schedulableBuilder = {
      // FAIR, FIFO, NONE 三种策略
      schedulingMode match {  
        case SchedulingMode.FIFO =>
          new FIFOSchedulableBuilder(rootPool)
        case SchedulingMode.FAIR =>
          new FairSchedulableBuilder(rootPool, sc)
        case _ =>
          throw new IllegalArgumentException(s"Unsupported $SCHEDULER_MODE_PROPERTY: " +
          s"$schedulingMode")
      }
    }
    schedulableBuilder.buildPools()
  }
```

## 1.4 backend.reviveOffers()

查看实现类 CoarseGrainedSchedulerBackend

CoarseGrainedSchedulerBackend.scala

```scala
class CoarseGrainedSchedulerBackend(scheduler: TaskSchedulerImpl, val rpcEnv: RpcEnv)
  extends ExecutorAllocationClient with SchedulerBackend with Logging {

  // 注册一个 RpcEndpoint ，返回它的 RpcEndpointRef
  // ENDPOINT_NAME = "CoarseGrainedScheduler"
  val driverEndpoint = rpcEnv.setupEndpoint(ENDPOINT_NAME, createDriverEndpoint())

  // Update the current offers and schedule tasks 
  override def reviveOffers(): Unit = Utils.tryLogNonFatalError {
    // 1.4.1节  driver给自己发送一条消息   ReviveOffers是driver的内部消息
    driverEndpoint.send(ReviveOffers)
  }
}
```
driver给自己发送了一条消息，那么它自己就得接收

CoarseGrainedSchedulerBackend.scala

```scala
class CoarseGrainedSchedulerBackend(scheduler: TaskSchedulerImpl, val rpcEnv: RpcEnv)
  extends ExecutorAllocationClient with SchedulerBackend with Logging {
    // driver接收消息
    override def receive: PartialFunction[Any, Unit] = {
     //...
      case ReviveOffers =>
        // 向 executors 发送任务，并通知 executors 启动任务
        makeOffers() 
      //...
    }

  // Make fake resource offers on all executors  
  // 向 executors 发送任务，并通知 executors 启动任务
  private def makeOffers(): Unit = {
    // Make sure no executor is killed while some task is launching on it
    val taskDescs = withLock {
      // 1.4.2节 过滤出活跃的 executor
      // Filter out executors under killing
      val activeExecutors = executorDataMap.filterKeys(isExecutorActive)
      // 对每个executor做转换
      val workOffers = activeExecutors.map {
        case (id, executorData) =>
          // 表示一个 executor 上空闲的可用资源  
          new WorkerOffer(id, executorData.executorHost, executorData.freeCores,
            Some(executorData.executorAddress.hostPort),
            executorData.resourcesInfo.map { case (rName, rInfo) =>
              (rName, rInfo.availableAddrs.toBuffer)
            }, executorData.resourceProfileId)
      }.toIndexedSeq
      // 1.4.3节 由集群管理器调用，以在workers上提供资源。
      // 响应方式是 按优先级顺序向活跃的 tasksets 请求任务。我们以轮询的方式将任务发送到每个节点上，以便在整个集群中平衡任务
      // 【这里就把任务发给节点了】
      scheduler.resourceOffers(workOffers, true)
    }
    if (taskDescs.nonEmpty) {
      // 启动任务（带有任务描述信息）
      launchTasks(taskDescs)
    }
  }

  // Launch tasks returned by a set of resource offers
  // 启动任务
  // 传入的参数是 TaskDescription （发给 executor 的待执行任务的描述，它通常由 TaskSetManager.resourceOffer 创建）
  private def launchTasks(tasks: Seq[Seq[TaskDescription]]): Unit = {
    // 遍历每个任务  
    // 这里的 task 是 TaskDescription
    for (task <- tasks.flatten) {
      // 序列化任务
      val serializedTask = TaskDescription.encode(task)
      // 判断任务的大小不能超过RPC通信发送消息的最大大小
      if (serializedTask.limit() >= maxRpcMessageSize) {
        Option(scheduler.taskIdToTaskSetManager.get(task.taskId)).foreach { taskSetMgr =>
          try {
            var msg = "Serialized task %s:%d was %d bytes, which exceeds max allowed: " +
              s"${RPC_MESSAGE_MAX_SIZE.key} (%d bytes). Consider increasing " +
              s"${RPC_MESSAGE_MAX_SIZE.key} or using broadcast variables for large values."
            msg = msg.format(task.taskId, task.index, serializedTask.limit(), maxRpcMessageSize)
            taskSetMgr.abort(msg)
          } catch {
            case e: Exception => logError("Exception in error callback", e)
          }
        }
      }
      else {
        //  取出这个任务所在的 executor 上的一些信息，比如 运行在的主机名、用于工作的核心数、可用资源的信息等
        val executorData = executorDataMap(task.executorId)
        // Do resources allocation here. The allocated resources will get released after the task finishes.
        // 为任务分配资源  
        val rpId = executorData.resourceProfileId  // 这个 executor 使用的 ResourceProfile 的id
        val prof = scheduler.sc.resourceProfileManager.resourceProfileFromId(rpId)
        val taskCpus = ResourceProfile.getTaskCpusOrDefaultForProfile(prof, conf)
        // 去掉被占用的核心数
        executorData.freeCores -= taskCpus
        task.resources.foreach { case (rName, rInfo) =>
          assert(executorData.resourcesInfo.contains(rName))
          // resourcesInfo: 当前，这个 executor 上可用资源的信息
          // acquire: 获取一个资源地址序列(为了启动的任务)，这些地址必须是可用的
          executorData.resourcesInfo(rName).acquire(rInfo.addresses)
        }

        logDebug(s"Launching task ${task.taskId} on executor id: ${task.executorId} hostname: " +
          s"${executorData.executorHost}.")
        // 给 executor 发消息，启动任务（带有任务描述信息）
        executorData.executorEndpoint.send(LaunchTask(new SerializableBuffer(serializedTask)))
      }
    }
  }
}
```

### 1.4.1 driverEndpoint.send(ReviveOffers)

```scala
private[spark] abstract class RpcEndpointRef(conf: SparkConf)
  extends Serializable with Logging {
  /**
   * Sends a one-way asynchronous message. Fire-and-forget semantics.
   */
  def send(message: Any): Unit
}
```

```scala
private[spark] object CoarseGrainedClusterMessages {
  // Internal messages in driver
  case object ReviveOffers extends CoarseGrainedClusterMessage
}
```

### 1.4.2 executorDataMap

CoarseGrainedSchedulerBackend.scala

```scala
  private val executorDataMap = new HashMap[String, ExecutorData]
```

```scala
/**
 * Grouping of data for an executor used by CoarseGrainedSchedulerBackend.
 * CoarseGrainedSchedulerBackend 使用的一个 executor 的数据分组
 *  
 * @param executorEndpoint The RpcEndpointRef representing this executor
 *                         表示这个 executor 的 RpcEndpointRef
 * @param executorAddress The network address of this executor
 *                        这个 executor 的网络地址
 * @param executorHost The hostname that this executor is running on
 *                     这个 executor 运行在的主机名
 * @param freeCores  The current number of cores available for work on the executor
 *                   当前，这个 executor 上可用于工作的核心数
 * @param totalCores The total number of cores available to the executor
 *                   这个 executor 上可用的核心总数
 * @param resourcesInfo The information of the currently available resources on the executor
 *                      当前，这个 executor 上可用资源的信息
 * @param resourceProfileId The id of the ResourceProfile being used by this executor
 *                          这个 executor 使用的 ResourceProfile 的id
 * @param registrationTs The registration timestamp of this executor
 *                       这个 executor 注册的时间戳
 */
private[cluster] class ExecutorData(
    val executorEndpoint: RpcEndpointRef,
    val executorAddress: RpcAddress,
    override val executorHost: String,
    var freeCores: Int,
    override val totalCores: Int,
    override val logUrlMap: Map[String, String],
    override val attributes: Map[String, String],
    override val resourcesInfo: Map[String, ExecutorResourceInfo],
    override val resourceProfileId: Int,
    val registrationTs: Long
) extends ExecutorInfo(executorHost, totalCores, logUrlMap, attributes,
  resourcesInfo, resourceProfileId)
```

### 1.4.3 scheduler.resourceOffers()

TaskSchedulerImpl.scala

```scala
private[spark] class TaskSchedulerImpl(
          val sc: SparkContext, val maxTaskFailures: Int,
          isLocal: Boolean = false, clock: Clock = new SystemClock)
  extends TaskScheduler with Logging {
  
  /**
   * Called by cluster manager to offer resources on workers. We respond by asking our active task
   * sets for tasks in order of priority. We fill each node with tasks in a round-robin manner so
   * that tasks are balanced across the cluster.
   * 由集群管理器调用，以在workers上提供资源。
   * 响应方式是 按优先级顺序向活跃的 tasksets 请求任务。我们以轮询的方式将任务发送到每个节点上，以便在整个集群中平衡任务
   */
  def resourceOffers(
           offers: IndexedSeq[WorkerOffer],
           isAllFreeResources: Boolean = true): Seq[Seq[TaskDescription]] = synchronized {
   //....
    // 1.4.2.1节  按照一定的算法调度TaskSetManager，也就是返回 TaskSetManager有序的一个集合，也就是反映了谁先执行谁后执行
    val sortedTaskSets = rootPool.getSortedTaskSetQueue

    // 按调度顺序取出每个 taskSet，然后按照位置等级增长的顺序提供给每个节点
    // Take each TaskSet in our scheduling order, and then offer it to each node in increasing order
    // of locality levels so that it gets a chance to launch local tasks on all of them.
    // NOTE: the preferredLocality order: PROCESS_LOCAL, NODE_LOCAL, NO_PREF, RACK_LOCAL, ANY
    for (taskSet <- sortedTaskSets) {
      //........
        // 1.4.2.2节 本地化级别  TODO
      // myLocalityLevels: 计算这个TaskSet中使用的本地化级别
        for (currentMaxLocality <- taskSet.myLocalityLevels) {
          // 循环计算全局最小的本地化级别
          var launchedTaskAtCurrentMaxLocality = false
          do {
            // 给  TaskSetManager 提供资源
            val (noDelayScheduleReject, minLocality) = resourceOfferSingleTaskSet(
              taskSet, currentMaxLocality, shuffledOffers, availableCpus,
              availableResources, tasks)
            // 启动任务的最小位置选项
            launchedTaskAtCurrentMaxLocality = minLocality.isDefined
            launchedAnyTask |= launchedTaskAtCurrentMaxLocality
            noDelaySchedulingRejects &= noDelayScheduleReject
            globalMinLocality = minTaskLocality(globalMinLocality, minLocality)
          } while (launchedTaskAtCurrentMaxLocality)
        }
      //....
    }
    // 返回任务
    return tasks.map(_.toSeq)
  }
}
```

#### 1.4.2.1 rootPool.getSortedTaskSetQueue

Pool.scala

```scala
  override def getSortedTaskSetQueue: ArrayBuffer[TaskSetManager] = {
    val sortedTaskSetQueue = new ArrayBuffer[TaskSetManager]
    // 使用提供的排序规则对 schedulableQueue 排序  （前面把TaskSetManager放入pool中了）
    val sortedSchedulableQueue =
      schedulableQueue.asScala.toSeq.sortWith(taskSetSchedulingAlgorithm.comparator)
    for (schedulable <- sortedSchedulableQueue) {
      sortedTaskSetQueue ++= schedulable.getSortedTaskSetQueue.filter(_.isSchedulable)
    }
    sortedTaskSetQueue
  }
   // 根据不同的调度策略，使用不同的排序规则
    private val taskSetSchedulingAlgorithm: SchedulingAlgorithm = {
      schedulingMode match {
        case SchedulingMode.FAIR =>
          new FairSchedulingAlgorithm()
        case SchedulingMode.FIFO =>
          new FIFOSchedulingAlgorithm()
        case _ =>
          val msg = s"Unsupported scheduling mode: $schedulingMode. Use FAIR or FIFO instead."
          throw new IllegalArgumentException(msg)
      }
    }
```

#### 1.4.2.2 taskSet.myLocalityLevels


```scala
private[spark] class TaskSchedulerImpl(
         val sc: SparkContext, val maxTaskFailures: Int,
         isLocal: Boolean = false, clock: Clock = new SystemClock)
  extends TaskScheduler with Logging {
  /**
   * Track the set of locality levels which are valid given the tasks locality preferences and
   * the set of currently available executors.  This is updated as executors are added and removed.
   * This allows a performance optimization, of skipping levels that aren't relevant (e.g., skip
   * PROCESS_LOCAL if no tasks could be run PROCESS_LOCAL for the current set of executors).
   * 基于给定任务的本地化首选项和当前可用的executors集合，跟踪有效的本地化级别的集合。
   * 随着 executors 的添加和删除，这将得到更新。
   * 这允许进行性能优化，跳过不相关的级别(例如，如果当前 executors 集合中没有任务可以运行PROCESS_LOCAL，则跳过PROCESS_LOCAL)。
   */
  // 计算这个TaskSet中使用的本地化级别
  private[scheduler] var myLocalityLevels = computeValidLocalityLevels()
}
```
TaskSetManager.scala

```scala
private[spark] class TaskSetManager(
       sched: TaskSchedulerImpl,val taskSet: TaskSet,
       val maxTaskFailures: Int,healthTracker: Option[HealthTracker] = None,
       clock: Clock = new SystemClock()) extends Schedulable with Logging {
  
  // Set of pending tasks for each executor.
  // 对每个 executor，挂起任务的集合
  val forExecutor = new HashMap[String, ArrayBuffer[Int]]
  
  // Store tasks waiting to be scheduled by locality preferences
  // 存储 等待被本地首选项调度的任务
  private[scheduler] val pendingTasks = new PendingTasksByLocality()

  /**
   * Compute the locality levels used in this TaskSet. Assumes that all tasks have already been
   * added to queues using addPendingTask.
   * 计算这个TaskSet中使用的本地化级别。
   * 假设所有的任务都使用 addPendingTask 方法添加到了队列中 
   */
  private def computeValidLocalityLevels(): Array[TaskLocality.TaskLocality] = {
    // Process local is expected to be used ONLY within TaskSetManager for now.
    /*
        进程本地化：数据和计算在同一个进程中
        节点本地化：数据和计算在同一个节点中
        机架本地化：数据和计算在同一个机架中
     */
    import TaskLocality.{PROCESS_LOCAL, NODE_LOCAL, NO_PREF, RACK_LOCAL, ANY}
    val levels = new ArrayBuffer[TaskLocality.TaskLocality]
    // 每个 executor 中挂起的任务不空，且 executor 都是活跃的
    if (!pendingTasks.forExecutor.isEmpty &&
      pendingTasks.forExecutor.keySet.exists(sched.isExecutorAlive(_))) {
      levels += PROCESS_LOCAL
    }
    if (!pendingTasks.forHost.isEmpty &&
      pendingTasks.forHost.keySet.exists(sched.hasExecutorsAliveOnHost(_))) {
      levels += NODE_LOCAL
    }
    if (!pendingTasks.noPrefs.isEmpty) {
      levels += NO_PREF
    }
    if (!pendingTasks.forRack.isEmpty &&
      pendingTasks.forRack.keySet.exists(sched.hasHostAliveOnRack(_))) {
      levels += RACK_LOCAL
    }
    levels += ANY
    logDebug("Valid locality levels for " + taskSet + ": " + levels.mkString(", "))
    levels.toArray
  }
}
```