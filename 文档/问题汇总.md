# 问题汇总

[TOC]

### 问题1

执行 `spark-submit wordcount.py`，出现了 `py4j.protocol.Py4JJavaError` 错误。

- 没启动 hadoop
- 文件目录设置错了。待读取的文件存放在本地，但这样设置 `/in/wc.txt` 。因为 spark 默认读的是 hdfs 中的数据。


### 问题2

本地执行 spark `WordCount.java`，报错`NoSuchMethodError: io.netty.buffer.PooledByteBufAllocator.defaultNumHeapArena`

io.netty 的 两个 jar 包导致的冲突，所以剔除 spark-core 中所有的低版本的 netty，再添加依赖高版本的：

```xml
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-core_2.11</artifactId>
    <version>2.4.4</version>
    <!--将netty包排除-->
    <exclusions>
        <exclusion>
         <groupId>io.netty</groupId>
            <artifactId>netty</artifactId>
        </exclusion>
    </exclusions>
    </dependency>
    <dependency>
        <groupId>io.netty</groupId>
        <artifactId>netty-all</artifactId>
        <version>4.1.42.Final</version>
    </dependency>
```


### 问题3

提交集群执行 `spark-submit --master spark://zgg:7077 --class sparkbase.wordcount hdfs://zgg:9000/in/wc.txt`，报错 `java.lang.ClassNotFoundException: sparkbase.wordcount`

解决方法见 `提交集群运行过程.md` 文档

### 问题4

当 `sc = SparkContext(conf)`，会报如下错误:
`org.apache.spark.SparkException: Could not parse Master URL: '<pyspark.conf.SparkConf object at 0x106666390>'`

解决：阅读代码，发现它的构造函数声明如下所示：

    def __init__(self, master=None, appName=None, sparkHome=None, pyFiles=None,
        environment=None, batchSize=0, serializer=PickleSerializer(), conf=None,
        gateway=None, jsc=None, profiler_cls=BasicProfiler):

而前面的代码仅仅是简单的将 conf 传递给 SparkContext 构造函数，这就会导致 Spark 会将 conf 看做是 master 参数的值，即默认为第一个参数。所以这里要带名参数：`sc = SparkContext(conf = conf)`

## 问题5

使用 `spark-submit` 执行程序时，出现 `ExitCodeException exitCode=13`，错误原因是搭建的 Spark 集群模式和在执行命令中指定的集群模式不一致。

