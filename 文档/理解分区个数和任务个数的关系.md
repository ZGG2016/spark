# 理解分区个数和任务个数的关系

一个 Stage 阶段中，最后一个 RDD 的分区个数就是 Task 的个数

**版本: spark3.3.0**

在 DAGScheduler 中，有一个叫 `submitStage` 方法。

```scala
private def submitStage(stage: Stage): Unit = {
    val jobId = activeJobForStage(stage)
    if (jobId.isDefined) {
      logDebug(s"submitStage($stage (name=${stage.name};" +
        s"jobs=${stage.jobIds.toSeq.sorted.mkString(",")}))")
      if (!waitingStages(stage) && !runningStages(stage) && !failedStages(stage)) {

      	// missing 理解成待计算
      	
      	// 取当前stage的父stage，要排序的，
      	// 一个stage处理完后，才能处理一个stage
        val missing = getMissingParentStages(stage).sortBy(_.id)
        logDebug("missing: " + missing)
        // 没有父stage，那就是当前stage就是待处理的stage
        if (missing.isEmpty) {
          logInfo("Submitting " + stage + " (" + stage.rdd + "), which has no missing parents")
          // 这里
          submitMissingTasks(stage, jobId.get)
        } 
        // 有父stage，那就按顺序处理父stage
        else {
          for (parent <- missing) {
            submitStage(parent)
          }
          waitingStages += stage
        }
      }
    } else {
      abortStage(stage, "No active job for stage " + stage.id, None)
    }
  }
```

```scala
private def submitMissingTasks(stage: Stage, jobId: Int): Unit = {
    ...
    val tasks: Seq[Task[_]] = try {
      val serializedTaskMetrics = closureSerializer.serialize(stage.latestInfo.taskMetrics).array()
      stage match {
        ...
        // 这里先只考虑只有一个 ResultStage 的情况
        // 这里调用 map 方法，在里面创建一个 ResultTask，
        // 所以，创建 ResultTask 的数量决定于调用map方法的次数，
        // 也就是 partitionsToCompute 的大小
        // partitionsToCompute 这个方法中，
        // 就是通过循环 job 的分区，返回待计算的分区id的列表
        case stage: ResultStage =>
          partitionsToCompute.map { id =>
            val p: Int = stage.partitions(id)
            val part = partitions(p)
            val locs = taskIdToLocations(id)
            new ResultTask(stage.id, stage.latestInfo.attemptNumber,
              taskBinary, part, locs, id, properties, serializedTaskMetrics,
              Option(jobId), Option(sc.applicationId), sc.applicationAttemptId,
              stage.rdd.isBarrier())
          }
      }
    } 
    ...
}
```

```scala
// Figure out the indexes of partition ids to compute.
val partitionsToCompute: Seq[Int] = stage.findMissingPartitions()
```

```
// stage.scala下
/** Returns the sequence of partition ids that are missing (i.e. needs to be computed). */
def findMissingPartitions(): Seq[Int]
```

找其实现类

```scala
/**
   * Returns the sequence of partition ids that are missing (i.e. needs to be computed).
   * 返回待计算的分区id的列表
   * 
   * This can only be called when there is an active job.
   */
  override def findMissingPartitions(): Seq[Int] = {
    val job = activeJob.get
    // 循环 job 的分区数量
    (0 until job.numPartitions).filter(id => !job.finished(id))
  }
```

----------------------------------------

根据 [尚硅谷 spark 教程](https://www.bilibili.com/video/BV11A411L7CK?p=97&vd_source=554a818fa3b2b8eac72efab5838146bf) 整理而来