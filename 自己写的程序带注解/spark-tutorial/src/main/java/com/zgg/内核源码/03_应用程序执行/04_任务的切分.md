
在 `03_阶段的划分.md` 中，分析划分阶段时，
在 `DAGScheduler.scala` 的 `handleJobSubmittedhandleJobSubmitted` 方法中创建了一个 `ResultStage`,
创建后会将它提交，在提交方法中就会切分任务。

DAGScheduler.scala

```scala
private[scheduler] def handleJobSubmitted(jobId: Int,
      finalRDD: RDD[_],
      func: (TaskContext, Iterator[_]) => _,
      partitions: Array[Int],
      callSite: CallSite,
      listener: JobListener,
      properties: Properties): Unit = {
    var finalStage: ResultStage = null
    try {
      // New stage creation may throw an exception if, for example, jobs are run on a
      // HadoopRDD whose underlying HDFS files have been deleted.
      // 创建一个和提供的 jobId 相关联的 ResultStage
      // rdd 就是 wordToSum  （还是以wordcount为例）
      finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)
    } catch {
      //....
    }
    // 提交阶段 
    // 参数是 ResultStage
    submitStage(finalStage)
  }

    /** Submits stage, but first recursively submits any missing parents. */
      // 提交stage, 但首先递归地提交丢失的父stage
      // 因为一个stage处理完后，才能处理下一个stage，所以要先找到还没计算过的（丢失的）stage
    private def submitStage(stage: Stage): Unit = {
      val jobId = activeJobForStage(stage)
      if (jobId.isDefined) {
        logDebug(s"submitStage($stage (name=${stage.name};" +
          s"jobs=${stage.jobIds.toSeq.sorted.mkString(",")}))")
        
        if (!waitingStages(stage) && !runningStages(stage) && !failedStages(stage)) {
          // 1.1节  把没计算过的（丢失的）stage以列表形式返回，然后再排序  （要依次计算）  
          // 【应用程序执行后，第一次执行到这里时，ResultStage的父stage肯定是还没计算的】
          val missing = getMissingParentStages(stage).sortBy(_.id)
          logDebug("missing: " + missing)
          // 第一次执行到这里时，ResultStage的父stage肯定是还没计算的，所以，missing是有值的
          // 1.2节 如果没有缺少的阶段，就提交这个阶段
          if (missing.isEmpty) {
            // 执行到这里面时，就先假定ShuffleMapStage已经执行完了
            logInfo("Submitting " + stage + " (" + stage.rdd + "), which has no missing parents")
            submitMissingTasks(stage, jobId.get)  // 参数是 ResultStage
          }
          // 第一次先执行这里【第一次执行到这里时，ResultStage的父stage肯定是还没计算的】
          else {
            // 依次处理每个尚未计算的stage
            for (parent <- missing) {
              // 【递归】
              // 以wordcount为例，这里传给 submitStage 的参数就是 ShuffleMapStage  
              // 把参数作为 ShuffleMapStage 代入submitStage方法分析  
              submitStage(parent)
            }
            waitingStages += stage
          }
        }
      } else {
        abortStage(stage, "No active job for stage " + stage.id, None)
      }
    }
```

## 1.1 getMissingParentStages()

DAGScheduler.scala

```scala
// 参数是 ResultStage 
// 把没计算过的（丢失的）stage以列表形式返回
private def getMissingParentStages(stage: Stage): List[Stage] = {
    val missing = new HashSet[Stage]
    val visited = new HashSet[RDD[_]]
    // We are manually maintaining a stack here to prevent StackOverflowError
    // caused by recursively visiting
    val waitingForVisit = new ListBuffer[RDD[_]]
    // 对于一个result stage, 它是运行action算子的RDD (wordToSum)
    waitingForVisit += stage.rdd
    def visit(rdd: RDD[_]): Unit = { // rdd (wordToSum)
      if (!visited(rdd)) {
        visited += rdd
        val rddHasUncachedPartitions = getCacheLocs(rdd).contains(Nil)
        if (rddHasUncachedPartitions) {
          for (dep <- rdd.dependencies) { // ShuffleDependency
            dep match {
              // 下面就是找到ShuffleMapStage，添加到 missing 这个HashSet里
              case shufDep: ShuffleDependency[_, _, _] =>
                // 取这个ShuffleDependency的 ShuffleMapStage
                val mapStage = getOrCreateShuffleMapStage(shufDep, stage.firstJobId)
                // Mark mapStage as available with shuffle outputs only after shuffle merge is
                // finalized with push based shuffle. If not, subsequent ShuffleMapStage won't
                // read from merged output as the MergeStatuses are not available.
                // 判断取到的 ShuffleMapStage 是不是没计算过的（丢失的）
                // 1.1.1节 条件1：mapStage.isAvailable: 当wordToOne分区数等于wordToSum分区数时，就返回true
                // 1.1.2节 条件2：TODO
                if (!mapStage.isAvailable || !mapStage.shuffleDep.shuffleMergeFinalized) {
                  missing += mapStage
                } else {
                  // Forward the nextAttemptId if skipped and get visited for the first time.
                  // Otherwise, once it gets retried,
                  // 1) the stuffs in stage info become distorting, e.g. task num, input byte, e.t.c
                  // 2) the first attempt starts from 0-idx, it will not be marked as a retry
                  mapStage.increaseAttemptIdOnFirstSkip()
                }
              case narrowDep: NarrowDependency[_] =>
                waitingForVisit.prepend(narrowDep.rdd)
            }
          }
        }
      }
    }
    while (waitingForVisit.nonEmpty) {
      visit(waitingForVisit.remove(0))
    }
    missing.toList
  }
```

### 1.1.1 mapStage.isAvailable

ShuffleMapStage.scala

```scala
 /**
   * Number of partitions that have shuffle outputs.
   * When this reaches [[numPartitions]], this map stage is ready.
   * 拥有shuffle输出的分区的数量
   * 当它等于 [[numPartitions]] 时，这个 map stage 就准备好了
   */
  def numAvailableOutputs: Int = mapOutputTrackerMaster.getNumAvailableOutputs(shuffleDep.shuffleId)

  /**
   * Returns true if the map stage is ready, i.e. all partitions have shuffle outputs.
   * wordToOne分区数等于wordToSum分区数时，就返回true
   */
    // val numPartitions = rdd.partitions.length  (wordToSum)
  def isAvailable: Boolean = numAvailableOutputs == numPartitions
```

MapOutputTrackerMaster.scala

```scala
/**
 * Driver-side class that keeps track of the location of the map output of a stage.
 * 追踪一个阶段的map输出的位置的 Driver端的类
 * 
 * The DAGScheduler uses this class to (de)register map output statuses and to look up statistics
 * for performing locality-aware reduce task scheduling.
 * DAGScheduler 使用这个类注册或取消注册map输出状态，查找执行位置感知的reduce任务调度的统计信息。
 * 
 * ShuffleMapStage uses this class for tracking available / missing outputs in order to determine
 * which tasks need to be run.
 * ShuffleMapStage 使用这个类追踪可用或缺少的输出，为了决定哪个任务需要被运行
 */
private[spark] class MapOutputTrackerMaster(
    conf: SparkConf,
    private[spark] val broadcastManager: BroadcastManager,
    private[spark] val isLocal: Boolean)
  extends MapOutputTracker(conf) {

      /**
       * Counter tracking the number of partitions that have output. This is a performance optimization
       * to avoid having to count the number of non-null entries in the `mapStatuses` array and should
       * be equivalent to`mapStatuses.count(_ ne null)`.
       * 计数器，追踪具有输出的分区的数量
       */
      private[this] var _numAvailableMapOutputs: Int = 0

      // HashMap for storing shuffleStatuses in the driver.
      // Statuses are dropped only by explicit de-registering.
      // Exposed for testing
      // 在driver存在shuffleStatuses的hashmap   key是shuffleId, value是ShuffleStatus
      val shuffleStatuses = new ConcurrentHashMap[Int, ShuffleStatus]().asScala

      /**
       * Number of partitions that have shuffle map outputs.
       * 具有 shuffle map 输出的分区数目
       */
      def numAvailableMapOutputs: Int = withReadLock {
        _numAvailableMapOutputs
      }  
  
      def getNumAvailableOutputs(shuffleId: Int): Int = {
        shuffleStatuses.get(shuffleId).map(_.numAvailableMapOutputs).getOrElse(0)
      }
  
  }
```

### 1.1.2 mapStage.shuffleDep.shuffleMergeFinalized  [TODO]

Dependency.scala

```scala
class ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag](
      @transient private val _rdd: RDD[_ <: Product2[K, V]],val partitioner: Partitioner,
      val serializer: Serializer = SparkEnv.get.serializer, val keyOrdering: Option[Ordering[K]] = None,
      val aggregator: Option[Aggregator[K, V, C]] = None, val mapSideCombine: Boolean = false,
      val shuffleWriterProcessor: ShuffleWriteProcessor = new ShuffleWriteProcessor)
  extends Dependency[Product2[K, V]] with Logging {
      // By default, shuffle merge is allowed for ShuffleDependency if push based shuffle is enabled
      // 默认，如果启用了基于push的shuffle, 那么对于ShuffleDependency, shuffle merge是允许的
      private[this] var _shuffleMergeAllowed = canShuffleMergeBeEnabled()
    
      def shuffleMergeEnabled: Boolean = shuffleMergeAllowed && mergerLocs.nonEmpty
    
      def shuffleMergeAllowed: Boolean = _shuffleMergeAllowed
    
      /**
       * Stores the location of the list of chosen external shuffle services for handling the
       * shuffle merge requests from mappers in this shuffle map stage.
       */
      private[spark] var mergerLocs: Seq[BlockManagerId] = Nil
    
      /**
       * Stores the information about whether the shuffle merge is finalized for the shuffle map stage
       * associated with this shuffle dependency
       */
      private[this] var _shuffleMergeFinalized: Boolean = false
    
      private[spark] def isShuffleMergeFinalizedMarked: Boolean = {
        _shuffleMergeFinalized
      }
    
      /**
       * Returns true if push-based shuffle is disabled or if the shuffle merge for
       * this shuffle is finalized.
       */
      def shuffleMergeFinalized: Boolean = {
        if (shuffleMergeEnabled) {
          isShuffleMergeFinalizedMarked
        } else {
          true
        }
      }
}
```

## 1.2 submitMissingTasks()

DAGScheduler.scala

```scala
/** Called when stage's parents are available and we can now do its task. */
  // 当 ResultStage 的父 ShuffleMapStage 可用时，调用这个方法，然后就可用执行它的任务了
  private def submitMissingTasks(stage: Stage, jobId: Int): Unit = {  // 参数是 ResultStage
    //....

    // Figure out the indexes of partition ids to compute. 
    // 1.2.1节 取到待计算的分区id的索引(ResultStage里的)  
    // stage是ResultStage时，当第一次执行到这里时，这个ResultStage中每个分区都还未计算，所以partitionsToCompute是有值的
    val partitionsToCompute: Seq[Int] = stage.findMissingPartitions()
    //....
    val tasks: Seq[Task[_]] = try {
      val serializedTaskMetrics = closureSerializer.serialize(stage.latestInfo.taskMetrics).array()
      stage match {
        case stage: ShuffleMapStage =>
          stage.pendingPartitions.clear()
          partitionsToCompute.map { id =>
            val locs = taskIdToLocations(id)
            val part = partitions(id)
            stage.pendingPartitions += id
            new ShuffleMapTask(stage.id, stage.latestInfo.attemptNumber,
              taskBinary, part, locs, properties, serializedTaskMetrics, Option(jobId),
              Option(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier())
          }
          
          // 所以，在这里将创建的各种任务（切分任务）
          
          /*
          这里先只考虑只有一个 ResultStage 的情况
            这里调用 map 方法，在里面创建一个 ResultTask，
            所以，创建 ResultTask 的数量决定于调用map方法的次数，也就是 partitionsToCompute 的大小
            partitionsToCompute 这个方法中，就是通过循环 job 的分区，再返回
           */
        case stage: ResultStage =>
          // 对每个待计算的分区，创建一个ResultTask任务
          partitionsToCompute.map { id =>
            val p: Int = stage.partitions(id)
            val part = partitions(p)
            val locs = taskIdToLocations(id)
            new ResultTask(stage.id, stage.latestInfo.attemptNumber,
              taskBinary, part, locs, id, properties, serializedTaskMetrics,
              Option(jobId), Option(sc.applicationId), sc.applicationAttemptId,
              stage.rdd.isBarrier())
          }
      }
    } catch {
     //...
    }

    if (tasks.nonEmpty) {
      logInfo(s"Submitting ${tasks.size} missing tasks from $stage (${stage.rdd}) (first 15 " +
        s"tasks are for partitions ${tasks.take(15).map(_.partitionId)})")
      // 提交一个任务序列来运行
      taskScheduler.submitTasks(new TaskSet(
        tasks.toArray, stage.id, stage.latestInfo.attemptNumber, jobId, properties,
        stage.resourceProfileId))
    } else {
      // Because we posted SparkListenerStageSubmitted earlier, we should mark
      // the stage as completed here in case there are no tasks to run
      markStageAsFinished(stage, None)
      stage match {
        case stage: ShuffleMapStage =>
          logDebug(s"Stage ${stage} is actually done; " +
              s"(available: ${stage.isAvailable}," +
              s"available outputs: ${stage.numAvailableOutputs}," +
              s"partitions: ${stage.numPartitions})")
          markMapStageJobsAsFinished(stage)
        case stage : ResultStage =>
          logDebug(s"Stage ${stage} is actually done; (partitions: ${stage.numPartitions})")
      }
      submitWaitingChildStages(stage)
    }
  }
```

### 1.2.1 stage.findMissingPartitions()

stage.scala

```scala
/** Returns the sequence of partition ids that are missing (i.e. needs to be computed). */
// 返回缺少的（例如待计算的）分区ids的序列
def findMissingPartitions(): Seq[Int]
```

找其实现类

ResultStage.scala

```scala
/**
   * Returns the sequence of partition ids that are missing (i.e. needs to be computed).
   * 返回缺少的（例如待计算的）分区ids的序列
   * 
   * This can only be called when there is an active job.
   */
  override def findMissingPartitions(): Seq[Int] = {
    val job = activeJob.get
    // 循环 job 每个分区，进行判断
    (0 until job.numPartitions).filter(id => !job.finished(id))
  }
```

ActiveJob.scala

```scala
    /**
     * Number of partitions we need to compute for this job. Note that result stages may not need
     * to compute all partitions in their target RDD, for actions like first() and lookup().
     * 对于这个job, 需要计算的分区的数量。
     * 注意： result stages 可能不需要计算目标RDD的所有分区，例如 first lookup
     */
    val numPartitions = finalStage match {
      // 如果是ResultStage, 分区数是 shuffle类算子产生的rdd的分区数（wordcount里就是wordToSum）
      case r: ResultStage => r.partitions.length
      // 如果是ShuffleMapStage, 分区数是调用shuffle类算子的rdd，也就是ShuffleMapStage的最后一个rdd的分区数
      case m: ShuffleMapStage => m.numPartitions
    }

  /** Which partitions of the stage have finished */
    // 这个阶段的哪个分区完成了
  val finished = Array.fill[Boolean](numPartitions)(false)
```

# 2 Task类

```scala
/**
 * A unit of execution. We have two kinds of Task's in Spark:
 * 一个执行单元。
 * 两种 Task 类型: ShuffleMapTask ResultTask
 *  - [[org.apache.spark.scheduler.ShuffleMapTask]]
 *  - [[org.apache.spark.scheduler.ResultTask]]
 *
 * A Spark job consists of one or more stages. The very last stage in a job consists of multiple
 * ResultTasks, while earlier stages consist of ShuffleMapTasks. A ResultTask executes the task
 * and sends the task output back to the driver application. A ShuffleMapTask executes the task
 * and divides the task output to multiple buckets (based on the task's partitioner).
 * 一个 Spark job 由一个或多个 stage 组成。
 * job 中最后一个 stage 由多个 ResultTasks 组成，而前面的 stages 由 ShuffleMapTasks 组成。
 * 一个 ResultTask 执行任务，并将任务输出发送回 driver 应用程序。
 * 一个 ShuffleMapTask 执行任务，并将任务输出划分到多个桶里（基于任务的分区器）
 * 
 * @param stageId id of the stage this task belongs to
 *                这个任务属于的 stage 的 id
 * @param stageAttemptId attempt id of the stage this task belongs to
 *                       这个任务属于的 stage 的尝试 id
 * @param partitionId index of the number in the RDD
 *                    RDD 中分区索引
 * @param localProperties copy of thread-local properties set by the user on the driver side.
 *                        在 driver 端，用户设置的 thread-local 属性的副本
 * @param serializedTaskMetrics a `TaskMetrics` that is created and serialized on the driver side
 *                              and sent to executor side.
 *                              在 driver 端，创建并序列化 TaskMetrics, 并发送到 executor 端
 *
 * The parameters below are optional:
 * 下面的参数是可选的：
 * @param jobId id of the job this task belongs to
 *              这个任务属于的 job 的 id
 * @param appId id of the app this task belongs to
 *              这个任务属于的应用程序的 id
 * @param appAttemptId attempt id of the app this task belongs to
 *                     这个任务属于的应用程序的尝试 id
 * @param isBarrier whether this task belongs to a barrier stage. Spark must launch all the tasks
 *                  at the same time for a barrier stage.
 *                  这个任务是否属于一个 barrier stage. 对于一个 barrier stage, Spark 必须同时启动所有的任务
 */
private[spark] abstract class Task[T](
    val stageId: Int,
    val stageAttemptId: Int,
    val partitionId: Int,
    @transient var localProperties: Properties = new Properties,
    // The default value is only used in tests.
    serializedTaskMetrics: Array[Byte] =
      SparkEnv.get.closureSerializer.newInstance().serialize(TaskMetrics.registered).array(),
    val jobId: Option[Int] = None,
    val appId: Option[String] = None,
    val appAttemptId: Option[String] = None,
    val isBarrier: Boolean = false) extends Serializable {}
```

## 2.1 ShuffleMapTask类

```scala
/**
 * A ShuffleMapTask divides the elements of an RDD into multiple buckets (based on a partitioner
 * specified in the ShuffleDependency).
 * 一个 ShuffleMapTask 将 RDD 的元素划分到多个桶里（基于在 ShuffleDependency 中指定的分区器）
 * 
 * See [[org.apache.spark.scheduler.Task]] for more information.
 *
 * @param stageId id of the stage this task belongs to
 *                这个任务属于的 stage 的 id
 * @param stageAttemptId attempt id of the stage this task belongs to
 *                       这个任务属于的 stage 的尝试 id
 * @param taskBinary broadcast version of the RDD and the ShuffleDependency. Once deserialized,
 *                   the type should be (RDD[_], ShuffleDependency[_, _, _]).
 *                   RDD 和 ShuffleDependency 的广播版本。 一旦被反序列化，类型就成为 (RDD[_], ShuffleDependency[_, _, _])
 * @param partition partition of the RDD this task is associated with
 *                  和这个任务关联的 RDD 的分区
 * @param locs preferred task execution locations for locality scheduling
 *             局部调度的首选任务执行位置
 * @param localProperties copy of thread-local properties set by the user on the driver side.
 *                        在 driver 端，用户设置的 thread-local 属性的副本
 * @param serializedTaskMetrics a `TaskMetrics` that is created and serialized on the driver side
 *                              and sent to executor side.
 *                              在 driver 端，创建并序列化 TaskMetrics, 并发送到 executor 端
 *
 * The parameters below are optional:
 * 下面的参数是可选的：
 * @param jobId id of the job this task belongs to
 *              这个任务属于的 job 的 id
 * @param appId id of the app this task belongs to
 *              这个任务属于的应用程序的 id
 * @param appAttemptId attempt id of the app this task belongs to
 *                     这个任务属于的应用程序的尝试 id
 * @param isBarrier whether this task belongs to a barrier stage. Spark must launch all the tasks
 *                  at the same time for a barrier stage.
 *                  这个任务是否属于一个 barrier stage. 对于一个 barrier stage, Spark 必须同时启动所有的任务
 */
private[spark] class ShuffleMapTask(
    stageId: Int,
    stageAttemptId: Int,
    taskBinary: Broadcast[Array[Byte]],
    partition: Partition,
    @transient private var locs: Seq[TaskLocation],
    localProperties: Properties,
    serializedTaskMetrics: Array[Byte],
    jobId: Option[Int] = None,
    appId: Option[String] = None,
    appAttemptId: Option[String] = None,
    isBarrier: Boolean = false)
  extends Task[MapStatus](stageId, stageAttemptId, partition.index, localProperties,
    serializedTaskMetrics, jobId, appId, appAttemptId, isBarrier)
  with Logging {}
```

## 2.2 ResultTask类

```scala
/**
 * A task that sends back the output to the driver application.
 * 将输出发送回 drvier 应用程序的任务
 * 
 * See [[Task]] for more information.
 *
 * @param stageId id of the stage this task belongs to
 *                这个任务属于的 stage 的 id
 * @param stageAttemptId attempt id of the stage this task belongs to
 *                       这个任务属于的 stage 的尝试 id
 * @param taskBinary broadcasted version of the serialized RDD and the function to apply on each
 *                   partition of the given RDD. Once deserialized, the type should be
 *                   (RDD[T], (TaskContext, Iterator[T]) => U).
 *                   序列化的 RDD ，以及在给定 RDD 的每个分区上应用的函数 的广播版本。
 *                   一旦被反序列化，类型就成为 (RDD[T], (TaskContext, Iterator[T]) => U)
 * @param partition partition of the RDD this task is associated with
 *                  和这个任务关联的 RDD 的分区
 * @param locs preferred task execution locations for locality scheduling
 *             局部调度的首选任务执行位置
 * @param outputId index of the task in this job (a job can launch tasks on only a subset of the
 *                 input RDD's partitions).
 *                 
 * @param localProperties copy of thread-local properties set by the user on the driver side.
 *                        在 driver 端，用户设置的 thread-local 属性的副本
 * @param serializedTaskMetrics a `TaskMetrics` that is created and serialized on the driver side
 *                              and sent to executor side.
 *                              在 driver 端，创建并序列化 TaskMetrics, 并发送到 executor 端
 *                              
 * The parameters below are optional:
 * 下面的参数是可选的：
 * @param jobId id of the job this task belongs to
 *              这个任务属于的 job 的 id
 * @param appId id of the app this task belongs to
 *              这个任务属于的应用程序的 id
 * @param appAttemptId attempt id of the app this task belongs to
 *                     这个任务属于的应用程序的尝试 id
 * @param isBarrier whether this task belongs to a barrier stage. Spark must launch all the tasks
 *                  at the same time for a barrier stage.
 *                  这个任务是否属于一个 barrier stage. 对于一个 barrier stage, Spark 必须同时启动所有的任务
 */
private[spark] class ResultTask[T, U](
                                       stageId: Int,
                                       stageAttemptId: Int,
                                       taskBinary: Broadcast[Array[Byte]],
                                       partition: Partition,
                                       locs: Seq[TaskLocation],
                                       val outputId: Int,
                                       localProperties: Properties,
                                       serializedTaskMetrics: Array[Byte],
                                       jobId: Option[Int] = None,
                                       appId: Option[String] = None,
                                       appAttemptId: Option[String] = None,
                                       isBarrier: Boolean = false)
  extends Task[U](stageId, stageAttemptId, partition.index, localProperties, serializedTaskMetrics,
    jobId, appId, appAttemptId, isBarrier)
    with Serializable {}
```